{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Sequence(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sequence, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(1, 51)\n",
    "        self.lstm2 = nn.LSTMCell(51, 51)\n",
    "        self.linear = nn.Linear(51, 1)\n",
    "\n",
    "    def forward(self, input, future = 0, seq_len = 32):\n",
    "        outputs = []\n",
    "        h_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        c_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        h_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        c_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "\n",
    "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
    "            h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            output = self.linear(h_t)\n",
    "            outputs += [output]\n",
    "        for i in range(future):# if we should predict the future\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            output = self.linear(h_t)\n",
    "            outputs += [output]\n",
    "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def make_data(directory=\"./train_data\"):\n",
    "    xTr = []\n",
    "    yTr = []\n",
    "    for filename in os.listdir(directory):\n",
    "        beats = np.load(directory + os.sep + filename)\n",
    "        x, y = split_data(beats)\n",
    "        xTr.append(x)\n",
    "        yTr.append(y)\n",
    "    xTr = np.concatenate(np.array(xTr))\n",
    "    yTr = np.concatenate(np.array(yTr))\n",
    "    return xTr, yTr\n",
    "    \n",
    "    \n",
    "def split_data(beats, data_len=32):\n",
    "    end = beats.shape[0]- (beats.shape[0]%(2*data_len))\n",
    "    beats = beats[:end].reshape(-1, 2*data_len)\n",
    "    x, y = beats[:,:32], beats[:,32:]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9861, 32) (9861, 32)\n"
     ]
    }
   ],
   "source": [
    "xTr, yTr = make_data()\n",
    "print(xTr.shape, yTr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:  0\n",
      "loss: 0.10727303510896695\n",
      "test loss: 0.10796947738200849\n",
      "STEP:  1\n",
      "loss: 0.1071526081953708\n",
      "test loss: 0.10780587471215781\n",
      "STEP:  2\n",
      "loss: 0.10698275826256122\n",
      "test loss: 0.10759977133457245\n",
      "STEP:  3\n",
      "loss: 0.10676818592580439\n",
      "test loss: 0.107354934241635\n",
      "STEP:  4\n",
      "loss: 0.1065127904819715\n",
      "test loss: 0.10707494092054311\n",
      "STEP:  5\n",
      "loss: 0.10622037695990272\n",
      "test loss: 0.10676322110497093\n",
      "STEP:  6\n",
      "loss: 0.10589464184025878\n",
      "test loss: 0.10642281591632491\n",
      "STEP:  7\n",
      "loss: 0.1055388798565359\n",
      "test loss: 0.10605626234537655\n",
      "STEP:  8\n",
      "loss: 0.10515583345035137\n",
      "test loss: 0.10566566589108001\n",
      "STEP:  9\n",
      "loss: 0.10474775167421073\n",
      "test loss: 0.10525282248210761\n",
      "STEP:  10\n",
      "loss: 0.10431651631132384\n",
      "test loss: 0.10481928010659751\n",
      "STEP:  11\n",
      "loss: 0.10386371847958939\n",
      "test loss: 0.1043663376013991\n",
      "STEP:  12\n",
      "loss: 0.10339067554288527\n",
      "test loss: 0.10389503075995302\n",
      "STEP:  13\n",
      "loss: 0.10289843267070048\n",
      "test loss: 0.10340614029830553\n",
      "STEP:  14\n",
      "loss: 0.10238778054964788\n",
      "test loss: 0.10290022192744694\n",
      "STEP:  15\n",
      "loss: 0.10185928909900381\n",
      "test loss: 0.10237764314043972\n",
      "STEP:  16\n",
      "loss: 0.10131334322094482\n",
      "test loss: 0.10183861592308219\n",
      "STEP:  17\n",
      "loss: 0.10075017200022579\n",
      "test loss: 0.10128322407952789\n",
      "STEP:  18\n",
      "loss: 0.10016987214759487\n",
      "test loss: 0.10071144791983096\n",
      "STEP:  19\n",
      "loss: 0.09957242988634568\n",
      "test loss: 0.10012318787263647\n",
      "STEP:  20\n",
      "loss: 0.09895774347185941\n",
      "test loss: 0.09951828660574907\n",
      "STEP:  21\n",
      "loss: 0.09832564584248686\n",
      "test loss: 0.09889654894851566\n",
      "STEP:  22\n",
      "loss: 0.09767592617380282\n",
      "test loss: 0.09825775980616673\n",
      "STEP:  23\n",
      "loss: 0.09700834986805286\n",
      "test loss: 0.09760170094012417\n",
      "STEP:  24\n",
      "loss: 0.09632267729354768\n",
      "test loss: 0.09692816739674721\n",
      "STEP:  25\n",
      "loss: 0.09561868172257204\n",
      "test loss: 0.0962369838531709\n",
      "STEP:  26\n",
      "loss: 0.09489616664617335\n",
      "test loss: 0.09552802073601784\n",
      "STEP:  27\n",
      "loss: 0.09415498241715307\n",
      "test loss: 0.0948012098484254\n",
      "STEP:  28\n",
      "loss: 0.09339504214610171\n",
      "test loss: 0.09405655930390927\n",
      "STEP:  29\n",
      "loss: 0.09261633684428203\n",
      "test loss: 0.09329416765204256\n",
      "STEP:  30\n",
      "loss: 0.09181894984068377\n",
      "test loss: 0.09251423712534486\n",
      "STEP:  31\n",
      "loss: 0.09100307046808105\n",
      "test loss: 0.09171708595068515\n",
      "STEP:  32\n",
      "loss: 0.09016900695836876\n",
      "test loss: 0.09090315967191527\n",
      "STEP:  33\n",
      "loss: 0.08931719844788504\n",
      "test loss: 0.0900730414225718\n",
      "STEP:  34\n",
      "loss: 0.08844822597275592\n",
      "test loss: 0.0892274610602578\n",
      "STEP:  35\n",
      "loss: 0.08756282231746294\n",
      "test loss: 0.0883673030301867\n",
      "STEP:  36\n",
      "loss: 0.08666188055763632\n",
      "test loss: 0.08749361277773327\n",
      "STEP:  37\n",
      "loss: 0.0857464611128459\n",
      "test loss: 0.0866076014924627\n",
      "STEP:  38\n",
      "loss: 0.08481779710193932\n",
      "test loss: 0.08571064894474544\n",
      "STEP:  39\n",
      "loss: 0.08387729777654304\n",
      "test loss: 0.08480430416986104\n",
      "STEP:  40\n",
      "loss: 0.0829265497979442\n",
      "test loss: 0.0838902837596413\n",
      "STEP:  41\n",
      "loss: 0.0819673161187053\n",
      "test loss: 0.08297046753541629\n",
      "STEP:  42\n",
      "loss: 0.08100153223511486\n",
      "test loss: 0.08204689139742354\n",
      "STEP:  43\n",
      "loss: 0.08003129959138747\n",
      "test loss: 0.08112173717526722\n",
      "STEP:  44\n",
      "loss: 0.07905887594265988\n",
      "test loss: 0.08019731934282999\n",
      "STEP:  45\n",
      "loss: 0.07808666252264013\n",
      "test loss: 0.07927606851093211\n",
      "STEP:  46\n",
      "loss: 0.07711718791271069\n",
      "test loss: 0.07836051167360554\n",
      "STEP:  47\n",
      "loss: 0.07615308857302057\n",
      "test loss: 0.07745324925959222\n",
      "STEP:  48\n",
      "loss: 0.07519708607212616\n",
      "test loss: 0.07655692912907267\n",
      "STEP:  49\n",
      "loss: 0.07425196113949145\n",
      "test loss: 0.07567421775458866\n",
      "STEP:  50\n",
      "loss: 0.07332052476305974\n",
      "test loss: 0.0748077689316245\n",
      "STEP:  51\n",
      "loss: 0.07240558666000668\n",
      "test loss: 0.07396019047448742\n",
      "STEP:  52\n",
      "loss: 0.07150992155906906\n",
      "test loss: 0.0731340094626359\n",
      "STEP:  53\n",
      "loss: 0.07063623384397215\n",
      "test loss: 0.07233163670648061\n",
      "STEP:  54\n",
      "loss: 0.06978712121374929\n",
      "test loss: 0.0715553311947804\n",
      "STEP:  55\n",
      "loss: 0.06896503811276798\n",
      "test loss: 0.07080716536239526\n",
      "STEP:  56\n",
      "loss: 0.06817225976384328\n",
      "test loss: 0.07008899207169549\n",
      "STEP:  57\n",
      "loss: 0.06741084769763\n",
      "test loss: 0.069402414227827\n",
      "STEP:  58\n",
      "loss: 0.06668261770355387\n",
      "test loss: 0.06874875794252633\n",
      "STEP:  59\n",
      "loss: 0.06598911112820997\n",
      "test loss: 0.06812905011943114\n",
      "STEP:  60\n",
      "loss: 0.06533157041179553\n",
      "test loss: 0.06754400125385711\n",
      "STEP:  61\n",
      "loss: 0.06471091967998033\n",
      "test loss: 0.06699399412158794\n",
      "STEP:  62\n",
      "loss: 0.06412775109725256\n",
      "test loss: 0.06647907887652778\n",
      "STEP:  63\n",
      "loss: 0.06358231753940742\n",
      "test loss: 0.06599897489047697\n",
      "STEP:  64\n",
      "loss: 0.06307453196268481\n",
      "test loss: 0.06555307945664333\n",
      "STEP:  65\n",
      "loss: 0.06260397363997182\n",
      "test loss: 0.06514048325045534\n",
      "STEP:  66\n",
      "loss: 0.06216990121046363\n",
      "test loss: 0.06475999220728403\n",
      "STEP:  67\n",
      "loss: 0.061771272257004343\n",
      "test loss: 0.06441015524829509\n",
      "STEP:  68\n",
      "loss: 0.06140676889744489\n",
      "test loss: 0.06408929707455681\n",
      "STEP:  69\n",
      "loss: 0.06107482866349082\n",
      "test loss: 0.06379555506727774\n",
      "STEP:  70\n",
      "loss: 0.060773679754524815\n",
      "test loss: 0.06352691918852363\n",
      "STEP:  71\n",
      "loss: 0.06050137960496575\n",
      "test loss: 0.06328127368016992\n",
      "STEP:  72\n",
      "loss: 0.06025585559947725\n",
      "test loss: 0.06305643931441678\n",
      "STEP:  73\n",
      "loss: 0.06003494671701559\n",
      "test loss: 0.06285021495918523\n",
      "STEP:  74\n",
      "loss: 0.0598364448843857\n",
      "test loss: 0.06266041728499623\n",
      "STEP:  75\n",
      "loss: 0.059658134872335164\n",
      "test loss: 0.062484917552236965\n",
      "STEP:  76\n",
      "loss: 0.05949783166802123\n",
      "test loss: 0.062321674571729456\n",
      "STEP:  77\n",
      "loss: 0.05935341440071869\n",
      "test loss: 0.06216876311786732\n",
      "STEP:  78\n",
      "loss: 0.05922285607328797\n",
      "test loss: 0.06202439728101582\n",
      "STEP:  79\n",
      "loss: 0.05910424855021842\n",
      "test loss: 0.06188694846276806\n",
      "STEP:  80\n",
      "loss: 0.05899582246186084\n",
      "test loss: 0.06175495793216261\n",
      "STEP:  81\n",
      "loss: 0.05889596189309966\n",
      "test loss: 0.06162714406252046\n",
      "STEP:  82\n",
      "loss: 0.05880321392202393\n",
      "test loss: 0.061502404547867776\n",
      "STEP:  83\n",
      "loss: 0.05871629325148418\n",
      "test loss: 0.061379814048068235\n",
      "STEP:  84\n",
      "loss: 0.05863408232656106\n",
      "test loss: 0.061258617827955025\n",
      "STEP:  85\n",
      "loss: 0.05855562744900146\n",
      "test loss: 0.0611382220359037\n",
      "STEP:  86\n",
      "loss: 0.05848013148335255\n",
      "test loss: 0.061018181311241274\n",
      "STEP:  87\n",
      "loss: 0.05840694379853741\n",
      "test loss: 0.060898184420094544\n",
      "STEP:  88\n",
      "loss: 0.058335548104686434\n",
      "test loss: 0.06077803859918414\n",
      "STEP:  89\n",
      "loss: 0.05826554883198335\n",
      "test loss: 0.06065765324190764\n",
      "STEP:  90\n",
      "loss: 0.05819665665995665\n",
      "test loss: 0.06053702349625401\n",
      "STEP:  91\n",
      "loss: 0.05812867374844293\n",
      "test loss: 0.06041621426567348\n",
      "STEP:  92\n",
      "loss: 0.058061479149763576\n",
      "test loss: 0.06029534501761436\n",
      "STEP:  93\n",
      "loss: 0.05799501480209561\n",
      "test loss: 0.060174575715308626\n",
      "STEP:  94\n",
      "loss: 0.05792927242068268\n",
      "test loss: 0.060054094101006573\n",
      "STEP:  95\n",
      "loss: 0.05786428152136094\n",
      "test loss: 0.05993410447689104\n",
      "STEP:  96\n",
      "loss: 0.05780009873335123\n",
      "test loss: 0.05981481805604516\n",
      "STEP:  97\n",
      "loss: 0.05773679848770443\n",
      "test loss: 0.05969644489190328\n",
      "STEP:  98\n",
      "loss: 0.05767446510661075\n",
      "test loss: 0.059579187341607734\n",
      "STEP:  99\n",
      "loss: 0.05761318626734541\n",
      "test loss: 0.05946323497683561\n",
      "STEP:  100\n",
      "loss: 0.057553047773912853\n",
      "test loss: 0.059348760824617486\n",
      "STEP:  101\n",
      "loss: 0.05749412953868757\n",
      "test loss: 0.05923591879971277\n",
      "STEP:  102\n",
      "loss: 0.057436502655107836\n",
      "test loss: 0.05912484217804726\n",
      "STEP:  103\n",
      "loss: 0.05738022742974243\n",
      "test loss: 0.05901564295634916\n",
      "STEP:  104\n",
      "loss: 0.05732535223669903\n",
      "test loss: 0.05890841194506332\n",
      "STEP:  105\n",
      "loss: 0.05727191305768712\n",
      "test loss: 0.05880321944844209\n",
      "STEP:  106\n",
      "loss: 0.05721993357669234\n",
      "test loss: 0.058700116396265524\n",
      "STEP:  107\n",
      "loss: 0.05716942570681884\n",
      "test loss: 0.0585991358046146\n",
      "STEP:  108\n",
      "loss: 0.05712039043841659\n",
      "test loss: 0.05850029445763466\n",
      "STEP:  109\n",
      "loss: 0.057072818910257506\n",
      "test loss: 0.05840359471730496\n",
      "STEP:  110\n",
      "loss: 0.05702669361918951\n",
      "test loss: 0.05830902638328869\n",
      "STEP:  111\n",
      "loss: 0.056981989697223526\n",
      "test loss: 0.05821656853939945\n",
      "STEP:  112\n",
      "loss: 0.05693867619793989\n",
      "test loss: 0.05812619133668014\n",
      "STEP:  113\n",
      "loss: 0.056896717346537055\n",
      "test loss: 0.05803785767534028\n",
      "STEP:  114\n",
      "loss: 0.05685607371877643\n",
      "test loss: 0.05795152475863918\n",
      "STEP:  115\n",
      "loss: 0.056816703324110214\n",
      "test loss: 0.05786714550119829\n",
      "STEP:  116\n",
      "loss: 0.05677856257657756\n",
      "test loss: 0.0577846697821463\n",
      "STEP:  117\n",
      "loss: 0.05674160714462575\n",
      "test loss: 0.05770404554006078\n",
      "STEP:  118\n",
      "loss: 0.056705792676571415\n",
      "test loss: 0.05762521971188579\n",
      "STEP:  119\n",
      "loss: 0.056671075403424545\n",
      "test loss: 0.05754813902206497\n",
      "STEP:  120\n",
      "loss: 0.05663741262435352\n",
      "test loss: 0.05747275063111634\n",
      "STEP:  121\n",
      "loss: 0.05660476308277304\n",
      "test loss: 0.05739900265495139\n",
      "STEP:  122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.05657308724283682\n",
      "test loss: 0.05732684456754123\n",
      "STEP:  123\n",
      "loss: 0.05654234747737378\n",
      "test loss: 0.05725622750017673\n",
      "STEP:  124\n",
      "loss: 0.056512508178661006\n",
      "test loss: 0.0571871044506817\n",
      "STEP:  125\n",
      "loss: 0.056483535803723534\n",
      "test loss: 0.057119430415661504\n",
      "STEP:  126\n",
      "loss: 0.05645539886523092\n",
      "test loss: 0.05705316245823605\n",
      "STEP:  127\n",
      "loss: 0.05642806787878952\n",
      "test loss: 0.0569882597228755\n",
      "STEP:  128\n",
      "loss: 0.05640151527636554\n",
      "test loss: 0.05692468340797483\n",
      "STEP:  129\n",
      "loss: 0.05637571529473196\n",
      "test loss: 0.05686239670568785\n",
      "STEP:  130\n",
      "loss: 0.05635064384685153\n",
      "test loss: 0.05680136471743435\n",
      "STEP:  131\n",
      "loss: 0.05632627838296442\n",
      "test loss: 0.05674155435237185\n",
      "STEP:  132\n",
      "loss: 0.05630259774718038\n",
      "test loss: 0.05668293421500131\n",
      "STEP:  133\n",
      "loss: 0.05627958203435394\n",
      "test loss: 0.05662547448707993\n",
      "STEP:  134\n",
      "loss: 0.05625721245106525\n",
      "test loss: 0.05656914680801053\n",
      "STEP:  135\n",
      "loss: 0.056235471183632965\n",
      "test loss: 0.0565139241570453\n",
      "STEP:  136\n",
      "loss: 0.05621434127538553\n",
      "test loss: 0.056459780739821906\n",
      "STEP:  137\n",
      "loss: 0.05619380651466138\n",
      "test loss: 0.05640669188109235\n",
      "STEP:  138\n",
      "loss: 0.056173851334405264\n",
      "test loss: 0.05635463392490184\n",
      "STEP:  139\n",
      "loss: 0.05615446072374406\n",
      "test loss: 0.056303584142979195\n",
      "STEP:  140\n",
      "loss: 0.05613562015153342\n",
      "test loss: 0.056253520651688316\n",
      "STEP:  141\n",
      "loss: 0.056117315501471664\n",
      "test loss: 0.0562044223375639\n",
      "STEP:  142\n",
      "loss: 0.05609953301817252\n",
      "test loss: 0.05615626879119087\n",
      "STEP:  143\n",
      "loss: 0.05608225926333516\n",
      "test loss: 0.05610904024899404\n",
      "STEP:  144\n",
      "loss: 0.05606548108113094\n",
      "test loss: 0.05606271754237715\n",
      "STEP:  145\n",
      "loss: 0.056049185571703256\n",
      "test loss: 0.05601728205354704\n",
      "STEP:  146\n",
      "loss: 0.05603336007177747\n",
      "test loss: 0.055972715677326236\n",
      "STEP:  147\n",
      "loss: 0.056017992141304554\n",
      "test loss: 0.05592900078823765\n",
      "STEP:  148\n",
      "loss: 0.05600306955510912\n",
      "test loss: 0.0558861202121578\n",
      "STEP:  149\n",
      "loss: 0.055988580298621\n",
      "test loss: 0.05584405720186619\n",
      "STEP:  150\n",
      "loss: 0.055974512566751386\n",
      "test loss: 0.05580279541587491\n",
      "STEP:  151\n",
      "loss: 0.055960854765145986\n",
      "test loss: 0.05576231889996796\n",
      "STEP:  152\n",
      "loss: 0.055947595513119376\n",
      "test loss: 0.05572261207095832\n",
      "STEP:  153\n",
      "loss: 0.05593472364760583\n",
      "test loss: 0.0556836597022217\n",
      "STEP:  154\n",
      "loss: 0.05592222822766\n",
      "test loss: 0.05564544691064456\n",
      "STEP:  155\n",
      "loss: 0.05591009853904945\n",
      "test loss: 0.05560795914467876\n",
      "STEP:  156\n",
      "loss: 0.05589832409863316\n",
      "test loss: 0.05557118217326003\n",
      "STEP:  157\n",
      "loss: 0.055886894658191584\n",
      "test loss: 0.05553510207539147\n",
      "STEP:  158\n",
      "loss: 0.05587580020762149\n",
      "test loss: 0.055499705230258135\n",
      "STEP:  159\n",
      "loss: 0.055865030977249916\n",
      "test loss: 0.05546497830775534\n",
      "STEP:  160\n",
      "loss: 0.05585457743932057\n",
      "test loss: 0.055430908259373\n",
      "STEP:  161\n",
      "loss: 0.055844430308500365\n",
      "test loss: 0.055397482309385515\n",
      "STEP:  162\n",
      "loss: 0.05583458054151409\n",
      "test loss: 0.05536468794633143\n",
      "STEP:  163\n",
      "loss: 0.0558250193358753\n",
      "test loss: 0.05533251291477904\n",
      "STEP:  164\n",
      "loss: 0.0558157381278216\n",
      "test loss: 0.055300945207381054\n",
      "STEP:  165\n",
      "loss: 0.05580672858951829\n",
      "test loss: 0.05526997305724162\n",
      "STEP:  166\n",
      "loss: 0.05579798262557599\n",
      "test loss: 0.055239584930603804\n",
      "STEP:  167\n",
      "loss: 0.05578949236907809\n",
      "test loss: 0.055209769519885855\n",
      "STEP:  168\n",
      "loss: 0.055781250177073825\n",
      "test loss: 0.055180515737074005\n",
      "STEP:  169\n",
      "loss: 0.05577324862575061\n",
      "test loss: 0.05515181270749509\n",
      "STEP:  170\n",
      "loss: 0.05576548050529543\n",
      "test loss: 0.05512364976397225\n",
      "STEP:  171\n",
      "loss: 0.05575793881457873\n",
      "test loss: 0.05509601644136338\n",
      "STEP:  172\n",
      "loss: 0.05575061675569446\n",
      "test loss: 0.05506890247149333\n",
      "STEP:  173\n",
      "loss: 0.055743507728437856\n",
      "test loss: 0.05504229777845484\n",
      "STEP:  174\n",
      "loss: 0.05573660532479046\n",
      "test loss: 0.055016192474283174\n",
      "STEP:  175\n",
      "loss: 0.055729903323429496\n",
      "test loss: 0.05499057685497039\n",
      "STEP:  176\n",
      "loss: 0.05572339568433892\n",
      "test loss: 0.05496544139681043\n",
      "STEP:  177\n",
      "loss: 0.05571707654351729\n",
      "test loss: 0.05494077675303927\n",
      "STEP:  178\n",
      "loss: 0.05571094020782206\n",
      "test loss: 0.05491657375075218\n",
      "STEP:  179\n",
      "loss: 0.05570498114996934\n",
      "test loss: 0.054892823388059474\n",
      "STEP:  180\n",
      "loss: 0.05569919400369433\n",
      "test loss: 0.05486951683145444\n",
      "STEP:  181\n",
      "loss: 0.05569357355907659\n",
      "test loss: 0.054846645413365466\n",
      "STEP:  182\n",
      "loss: 0.055688114758036464\n",
      "test loss: 0.05482420062985567\n",
      "STEP:  183\n",
      "loss: 0.05568281269000139\n",
      "test loss: 0.054802174138443946\n",
      "STEP:  184\n",
      "loss: 0.055677662587712645\n",
      "test loss: 0.05478055775602209\n",
      "STEP:  185\n",
      "loss: 0.05567265982322413\n",
      "test loss: 0.054759343456839346\n",
      "STEP:  186\n",
      "loss: 0.05566779990401419\n",
      "test loss: 0.054738523370529445\n",
      "STEP:  187\n",
      "loss: 0.05566307846924009\n",
      "test loss: 0.05471808978016174\n",
      "STEP:  188\n",
      "loss: 0.055658491286133746\n",
      "test loss: 0.054698035120299894\n",
      "STEP:  189\n",
      "loss: 0.05565403424650646\n",
      "test loss: 0.05467835197504907\n",
      "STEP:  190\n",
      "loss: 0.05564970336334744\n",
      "test loss: 0.05465903307607709\n",
      "STEP:  191\n",
      "loss: 0.05564549476756928\n",
      "test loss: 0.054640071300607496\n",
      "STEP:  192\n",
      "loss: 0.05564140470478109\n",
      "test loss: 0.054621459669369384\n",
      "STEP:  193\n",
      "loss: 0.05563742953218228\n",
      "test loss: 0.05460319134449794\n",
      "STEP:  194\n",
      "loss: 0.0556335657155475\n",
      "test loss: 0.05458525962739019\n",
      "STEP:  195\n",
      "loss: 0.055629809826266786\n",
      "test loss: 0.05456765795651052\n",
      "STEP:  196\n",
      "loss: 0.055626158538431704\n",
      "test loss: 0.05455037990514233\n",
      "STEP:  197\n",
      "loss: 0.0556226086260233\n",
      "test loss: 0.05453341917909611\n",
      "STEP:  198\n",
      "loss: 0.05561915696015581\n",
      "test loss: 0.054516769614373635\n",
      "STEP:  199\n",
      "loss: 0.05561580050635152\n",
      "test loss: 0.054500425174788696\n",
      "STEP:  200\n",
      "loss: 0.055612536321912744\n",
      "test loss: 0.05448437994956115\n",
      "STEP:  201\n",
      "loss: 0.0556093615533018\n",
      "test loss: 0.05446862815087566\n",
      "STEP:  202\n",
      "loss: 0.05560627343362471\n",
      "test loss: 0.054453164111423896\n",
      "STEP:  203\n",
      "loss: 0.0556032692801484\n",
      "test loss: 0.05443798228193294\n",
      "STEP:  204\n",
      "loss: 0.055600346491847724\n",
      "test loss: 0.05442307722868051\n",
      "STEP:  205\n",
      "loss: 0.055597502547064574\n",
      "test loss: 0.05440844363101435\n",
      "STEP:  206\n",
      "loss: 0.055594735001155096\n",
      "test loss: 0.054394076278877324\n",
      "STEP:  207\n",
      "loss: 0.05559204148425135\n",
      "test loss: 0.05437997007034256\n",
      "STEP:  208\n",
      "loss: 0.05558941969902837\n",
      "test loss: 0.05436612000916765\n",
      "STEP:  209\n",
      "loss: 0.055586867418570066\n",
      "test loss: 0.054352521202373404\n",
      "STEP:  210\n",
      "loss: 0.055584382484243244\n",
      "test loss: 0.05433916885785353\n",
      "STEP:  211\n",
      "loss: 0.05558196280367876\n",
      "test loss: 0.05432605828201478\n",
      "STEP:  212\n",
      "loss: 0.055579606348756694\n",
      "test loss: 0.054313184877459016\n",
      "STEP:  213\n",
      "loss: 0.05557731115367923\n",
      "test loss: 0.05430054414070629\n",
      "STEP:  214\n",
      "loss: 0.05557507531309048\n",
      "test loss: 0.054288131659961714\n",
      "STEP:  215\n",
      "loss: 0.055572896980240385\n",
      "test loss: 0.05427594311293267\n",
      "STEP:  216\n",
      "loss: 0.05557077436521349\n",
      "test loss: 0.054263974264696466\n",
      "STEP:  217\n",
      "loss: 0.05556870573319695\n",
      "test loss: 0.054252220965618415\n",
      "STEP:  218\n",
      "loss: 0.05556668940281841\n",
      "test loss: 0.05424067914932253\n",
      "STEP:  219\n",
      "loss: 0.055564723744508195\n",
      "test loss: 0.05422934483072166\n",
      "STEP:  220\n",
      "loss: 0.05556280717894482\n",
      "test loss: 0.054218214104096274\n",
      "STEP:  221\n",
      "loss: 0.055560938175495365\n",
      "test loss: 0.054207283141230266\n",
      "STEP:  222\n",
      "loss: 0.05555911525078537\n",
      "test loss: 0.05419654818960585\n",
      "STEP:  223\n",
      "loss: 0.05555733696721694\n",
      "test loss: 0.05418600557064386\n",
      "STEP:  224\n",
      "loss: 0.05555560193160341\n",
      "test loss: 0.05417565167800717\n",
      "STEP:  225\n",
      "loss: 0.05555390879383134\n",
      "test loss: 0.05416548297595011\n",
      "STEP:  226\n",
      "loss: 0.055552256245529646\n",
      "test loss: 0.05415549599772271\n",
      "STEP:  227\n",
      "loss: 0.05555064301883452\n",
      "test loss: 0.05414568734402301\n",
      "STEP:  228\n",
      "loss: 0.05554906788513958\n",
      "test loss: 0.05413605368150076\n",
      "STEP:  229\n",
      "loss: 0.05554752965392044\n",
      "test loss: 0.0541265917413074\n",
      "STEP:  230\n",
      "loss: 0.055546027171601876\n",
      "test loss: 0.05411729831768918\n",
      "STEP:  231\n",
      "loss: 0.05554455932039922\n",
      "test loss: 0.05410817026663005\n",
      "STEP:  232\n",
      "loss: 0.05554312501728512\n",
      "test loss: 0.05409920450452953\n",
      "STEP:  233\n",
      "loss: 0.055541723212904945\n",
      "test loss: 0.054090398006928844\n",
      "STEP:  234\n",
      "loss: 0.055540352890587216\n",
      "test loss: 0.05408174780727049\n",
      "STEP:  235\n",
      "loss: 0.05553901306535126\n",
      "test loss: 0.054073250995696966\n",
      "STEP:  236\n",
      "loss: 0.05553770278293818\n",
      "test loss: 0.05406490471788354\n",
      "STEP:  237\n",
      "loss: 0.055536421118905756\n",
      "test loss: 0.054056706173909536\n",
      "STEP:  238\n",
      "loss: 0.05553516717771167\n",
      "test loss: 0.05404865261715493\n",
      "STEP:  239\n",
      "loss: 0.05553394009185723\n",
      "test loss: 0.054040741353232565\n",
      "STEP:  240\n",
      "loss: 0.05553273902102908\n",
      "test loss: 0.05403296973894856\n",
      "STEP:  241\n",
      "loss: 0.055531563151285115\n",
      "test loss: 0.05402533518128833\n",
      "STEP:  242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.055530411694257015\n",
      "test loss: 0.054017835136431616\n",
      "STEP:  243\n",
      "loss: 0.05552928388637604\n",
      "test loss: 0.05401046710878871\n",
      "STEP:  244\n",
      "loss: 0.05552817898812515\n",
      "test loss: 0.0540032286500676\n",
      "STEP:  245\n",
      "loss: 0.055527096283297606\n",
      "test loss: 0.05399611735835316\n",
      "STEP:  246\n",
      "loss: 0.05552603507831506\n",
      "test loss: 0.05398913087721717\n",
      "STEP:  247\n",
      "loss: 0.05552499470151311\n",
      "test loss: 0.0539822668948441\n",
      "STEP:  248\n",
      "loss: 0.055523974502502835\n",
      "test loss: 0.05397552314317653\n",
      "STEP:  249\n",
      "loss: 0.05552297385148642\n",
      "test loss: 0.053968897397078275\n",
      "STEP:  250\n",
      "loss: 0.05552199213867421\n",
      "test loss: 0.053962387473519745\n",
      "STEP:  251\n",
      "loss: 0.05552102877363057\n",
      "test loss: 0.053955991230774096\n",
      "STEP:  252\n",
      "loss: 0.05552008318471392\n",
      "test loss: 0.053949706567632996\n",
      "STEP:  253\n",
      "loss: 0.05551915481848516\n",
      "test loss: 0.05394353142263684\n",
      "STEP:  254\n",
      "loss: 0.055518243139143476\n",
      "test loss: 0.053937463773320225\n",
      "STEP:  255\n",
      "loss: 0.05551734762800238\n",
      "test loss: 0.053931501635473354\n",
      "STEP:  256\n",
      "loss: 0.05551646778295121\n",
      "test loss: 0.05392564306241152\n",
      "STEP:  257\n",
      "loss: 0.055515603117937844\n",
      "test loss: 0.053919886144266675\n",
      "STEP:  258\n",
      "loss: 0.05551475316247905\n",
      "test loss: 0.053914229007283695\n",
      "STEP:  259\n",
      "loss: 0.05551391746117967\n",
      "test loss: 0.05390866981313508\n",
      "STEP:  260\n",
      "loss: 0.055513095573256614\n",
      "test loss: 0.0539032067582445\n",
      "STEP:  261\n",
      "loss: 0.055512287072104594\n",
      "test loss: 0.05389783807312442\n",
      "STEP:  262\n",
      "loss: 0.05551149154481643\n",
      "test loss: 0.053892562021723725\n",
      "STEP:  263\n",
      "loss: 0.05551070859178012\n",
      "test loss: 0.053887376900789076\n",
      "STEP:  264\n",
      "loss: 0.055509937826274655\n",
      "test loss: 0.053882281039233614\n",
      "STEP:  265\n",
      "loss: 0.055509178874027044\n",
      "test loss: 0.05387727279752144\n",
      "STEP:  266\n",
      "loss: 0.055508431372858685\n",
      "test loss: 0.05387235056706023\n",
      "STEP:  267\n",
      "loss: 0.05550769497228687\n",
      "test loss: 0.053867512769605795\n",
      "STEP:  268\n",
      "loss: 0.05550696933315243\n",
      "test loss: 0.053862757856673535\n",
      "STEP:  269\n",
      "loss: 0.05550625412726735\n",
      "test loss: 0.05385808430896653\n",
      "STEP:  270\n",
      "loss: 0.05550554903706668\n",
      "test loss: 0.0538534906358096\n",
      "STEP:  271\n",
      "loss: 0.055504853755269194\n",
      "test loss: 0.053848975374594286\n",
      "STEP:  272\n",
      "loss: 0.055504167984555174\n",
      "test loss: 0.05384453709023369\n",
      "STEP:  273\n",
      "loss: 0.055503491437238074\n",
      "test loss: 0.053840174374630234\n",
      "STEP:  274\n",
      "loss: 0.05550282383496697\n",
      "test loss: 0.05383588584614831\n",
      "STEP:  275\n",
      "loss: 0.055502164908427715\n",
      "test loss: 0.05383167014909992\n",
      "STEP:  276\n",
      "loss: 0.05550151439703833\n",
      "test loss: 0.053827525953240724\n",
      "STEP:  277\n",
      "loss: 0.05550087204869944\n",
      "test loss: 0.053823451953272185\n",
      "STEP:  278\n",
      "loss: 0.05550023761948305\n",
      "test loss: 0.05381944686835612\n",
      "STEP:  279\n",
      "loss: 0.05549961087339244\n",
      "test loss: 0.05381550944163806\n",
      "STEP:  280\n",
      "loss: 0.055498991582111946\n",
      "test loss: 0.05381163843977871\n",
      "STEP:  281\n",
      "loss: 0.05549837952473779\n",
      "test loss: 0.05380783265249588\n",
      "STEP:  282\n",
      "loss: 0.05549777448755982\n",
      "test loss: 0.053804090892114444\n",
      "STEP:  283\n",
      "loss: 0.05549717626380793\n",
      "test loss: 0.053800411993125144\n",
      "STEP:  284\n",
      "loss: 0.055496584653440484\n",
      "test loss: 0.053796794811753694\n",
      "STEP:  285\n",
      "loss: 0.05549599946292773\n",
      "test loss: 0.05379323822553592\n",
      "STEP:  286\n",
      "loss: 0.0554954205050284\n",
      "test loss: 0.05378974113290493\n",
      "STEP:  287\n",
      "loss: 0.05549484759859232\n",
      "test loss: 0.05378630245278299\n",
      "STEP:  288\n",
      "loss: 0.055494280568370684\n",
      "test loss: 0.05378292112418422\n",
      "STEP:  289\n",
      "loss: 0.055493719244791954\n",
      "test loss: 0.05377959610582409\n",
      "STEP:  290\n",
      "loss: 0.0554931634638278\n",
      "test loss: 0.05377632637573811\n",
      "STEP:  291\n",
      "loss: 0.055492613066757006\n",
      "test loss: 0.05377311093090738\n",
      "STEP:  292\n",
      "loss: 0.05549206790003673\n",
      "test loss: 0.05376994878689138\n",
      "STEP:  293\n",
      "loss: 0.05549152781509924\n",
      "test loss: 0.0537668389774722\n",
      "STEP:  294\n",
      "loss: 0.05549099266820948\n",
      "test loss: 0.053763780554298875\n",
      "STEP:  295\n",
      "loss: 0.05549046232030981\n",
      "test loss: 0.05376077258654682\n",
      "STEP:  296\n",
      "loss: 0.055489936636848654\n",
      "test loss: 0.05375781416057972\n",
      "STEP:  297\n",
      "loss: 0.05548941548764258\n",
      "test loss: 0.053754904379619266\n",
      "STEP:  298\n",
      "loss: 0.05548889874673231\n",
      "test loss: 0.053752042363422375\n",
      "STEP:  299\n",
      "loss: 0.05548838629225152\n",
      "test loss: 0.05374922724796691\n",
      "STEP:  300\n",
      "loss: 0.055487878006279495\n",
      "test loss: 0.05374645818513911\n",
      "STEP:  301\n",
      "loss: 0.05548737377471718\n",
      "test loss: 0.05374373434243246\n",
      "STEP:  302\n",
      "loss: 0.055486873487163524\n",
      "test loss: 0.05374105490264821\n",
      "STEP:  303\n",
      "loss: 0.05548637703679601\n",
      "test loss: 0.053738419063610286\n",
      "STEP:  304\n",
      "loss: 0.0554858843202453\n",
      "test loss: 0.05373582603787285\n",
      "STEP:  305\n",
      "loss: 0.055485395237482395\n",
      "test loss: 0.0537332750524485\n",
      "STEP:  306\n",
      "loss: 0.05548490969172281\n",
      "test loss: 0.0537307653485303\n",
      "STEP:  307\n",
      "loss: 0.05548442758929505\n",
      "test loss: 0.05372829618122729\n",
      "STEP:  308\n",
      "loss: 0.055483948839568475\n",
      "test loss: 0.0537258668193007\n",
      "STEP:  309\n",
      "loss: 0.0554834733548259\n",
      "test loss: 0.05372347654490903\n",
      "STEP:  310\n",
      "loss: 0.05548300105018526\n",
      "test loss: 0.05372112465335428\n",
      "STEP:  311\n",
      "loss: 0.05548253184349482\n",
      "test loss: 0.05371881045283873\n",
      "STEP:  312\n",
      "loss: 0.05548206565525685\n",
      "test loss: 0.05371653326422074\n",
      "STEP:  313\n",
      "loss: 0.055481602408530555\n",
      "test loss: 0.05371429242078043\n",
      "STEP:  314\n",
      "loss: 0.05548114202883961\n",
      "test loss: 0.0537120872679866\n",
      "STEP:  315\n",
      "loss: 0.055480684444117995\n",
      "test loss: 0.05370991716326988\n",
      "STEP:  316\n",
      "loss: 0.05548022958460268\n",
      "test loss: 0.05370778147580171\n",
      "STEP:  317\n",
      "loss: 0.055479777382770816\n",
      "test loss: 0.05370567958627365\n",
      "STEP:  318\n",
      "loss: 0.05547932777327508\n",
      "test loss: 0.05370361088668554\n",
      "STEP:  319\n",
      "loss: 0.055478880692843256\n",
      "test loss: 0.05370157478013571\n",
      "STEP:  320\n",
      "loss: 0.05547843608024935\n",
      "test loss: 0.0536995706806141\n",
      "STEP:  321\n",
      "loss: 0.05547799387620957\n",
      "test loss: 0.05369759801280316\n",
      "STEP:  322\n",
      "loss: 0.05547755402334337\n",
      "test loss: 0.053695656211877336\n",
      "STEP:  323\n",
      "loss: 0.055477116466096765\n",
      "test loss: 0.05369374472331189\n",
      "STEP:  324\n",
      "loss: 0.05547668115069323\n",
      "test loss: 0.05369186300269222\n",
      "STEP:  325\n",
      "loss: 0.05547624802506129\n",
      "test loss: 0.05369001051552627\n",
      "STEP:  326\n",
      "loss: 0.055475817038803624\n",
      "test loss: 0.05368818673706364\n",
      "STEP:  327\n",
      "loss: 0.055475388143105576\n",
      "test loss: 0.053686391152115744\n",
      "STEP:  328\n",
      "loss: 0.05547496129071631\n",
      "test loss: 0.053684623254879806\n",
      "STEP:  329\n",
      "loss: 0.055474536435875355\n",
      "test loss: 0.053682882548766145\n",
      "STEP:  330\n",
      "loss: 0.055474113534278946\n",
      "test loss: 0.0536811685462296\n",
      "STEP:  331\n",
      "loss: 0.05547369254302907\n",
      "test loss: 0.05367948076860335\n",
      "STEP:  332\n",
      "loss: 0.05547327342057189\n",
      "test loss: 0.05367781874593528\n",
      "STEP:  333\n",
      "loss: 0.0554728561266795\n",
      "test loss: 0.05367618201683099\n",
      "STEP:  334\n",
      "loss: 0.05547244062237925\n",
      "test loss: 0.05367457012829255\n",
      "STEP:  335\n",
      "loss: 0.05547202686994281\n",
      "test loss: 0.053672982635568973\n",
      "STEP:  336\n",
      "loss: 0.055471614832809074\n",
      "test loss: 0.05367141910200265\n",
      "STEP:  337\n",
      "loss: 0.0554712044755814\n",
      "test loss: 0.05366987909888322\n",
      "STEP:  338\n",
      "loss: 0.055470795763965575\n",
      "test loss: 0.05366836220529924\n",
      "STEP:  339\n",
      "loss: 0.05547038866474292\n",
      "test loss: 0.053666868007999226\n",
      "STEP:  340\n",
      "loss: 0.055469983145742564\n",
      "test loss: 0.053665396101249346\n",
      "STEP:  341\n",
      "loss: 0.055469579175781304\n",
      "test loss: 0.05366394608669624\n",
      "STEP:  342\n",
      "loss: 0.05546917672466443\n",
      "test loss: 0.05366251757323304\n",
      "STEP:  343\n",
      "loss: 0.05546877576312367\n",
      "test loss: 0.05366111017686679\n",
      "STEP:  344\n",
      "loss: 0.05546837626281021\n",
      "test loss: 0.05365972352058765\n",
      "STEP:  345\n",
      "loss: 0.055467978196255714\n",
      "test loss: 0.053658357234243954\n",
      "STEP:  346\n",
      "loss: 0.05546758153683486\n",
      "test loss: 0.05365701095441313\n",
      "STEP:  347\n",
      "loss: 0.05546718625875003\n",
      "test loss: 0.05365568432428333\n",
      "STEP:  348\n",
      "loss: 0.05546679233700586\n",
      "test loss: 0.053654376993529346\n",
      "STEP:  349\n",
      "loss: 0.05546639974736139\n",
      "test loss: 0.053653088618197044\n",
      "STEP:  350\n",
      "loss: 0.055466008466335724\n",
      "test loss: 0.053651818860585114\n",
      "STEP:  351\n",
      "loss: 0.05546561847116238\n",
      "test loss: 0.053650567389132646\n",
      "STEP:  352\n",
      "loss: 0.055465229739777\n",
      "test loss: 0.05364933387830633\n",
      "STEP:  353\n",
      "loss: 0.055464842250780036\n",
      "test loss: 0.053648118008491535\n",
      "STEP:  354\n",
      "loss: 0.05546445598343366\n",
      "test loss: 0.05364691946588362\n",
      "STEP:  355\n",
      "loss: 0.0554640709176316\n",
      "test loss: 0.05364573794238362\n",
      "STEP:  356\n",
      "loss: 0.05546368703387234\n",
      "test loss: 0.05364457313549233\n",
      "STEP:  357\n",
      "loss: 0.05546330431324665\n",
      "test loss: 0.05364342474820985\n",
      "STEP:  358\n",
      "loss: 0.055462922737424404\n",
      "test loss: 0.05364229248893591\n",
      "STEP:  359\n",
      "loss: 0.05546254228861566\n",
      "test loss: 0.05364117607137023\n",
      "STEP:  360\n",
      "loss: 0.05546216294957929\n",
      "test loss: 0.05364007521441747\n",
      "STEP:  361\n",
      "loss: 0.05546178470358011\n",
      "test loss: 0.05363898964209209\n",
      "STEP:  362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0554614075343949\n",
      "test loss: 0.053637919083425134\n",
      "STEP:  363\n",
      "loss: 0.05546103142627697\n",
      "test loss: 0.053636863272373166\n",
      "STEP:  364\n",
      "loss: 0.0554606563639548\n",
      "test loss: 0.05363582194772997\n",
      "STEP:  365\n",
      "loss: 0.05546028233260286\n",
      "test loss: 0.053634794853037314\n",
      "STEP:  366\n",
      "loss: 0.055459909317847576\n",
      "test loss: 0.05363378173649922\n",
      "STEP:  367\n",
      "loss: 0.05545953730572894\n",
      "test loss: 0.053632782350897926\n",
      "STEP:  368\n",
      "loss: 0.05545916628270177\n",
      "test loss: 0.05363179645350844\n",
      "STEP:  369\n",
      "loss: 0.05545879623561783\n",
      "test loss: 0.05363082380602084\n",
      "STEP:  370\n",
      "loss: 0.05545842715172714\n",
      "test loss: 0.053629864174456546\n",
      "STEP:  371\n",
      "loss: 0.055458059018633314\n",
      "test loss: 0.05362891732909274\n",
      "STEP:  372\n",
      "loss: 0.05545769182430315\n",
      "test loss: 0.053627983044381995\n",
      "STEP:  373\n",
      "loss: 0.055457325557072515\n",
      "test loss: 0.0536270610988793\n",
      "STEP:  374\n",
      "loss: 0.0554569602056031\n",
      "test loss: 0.053626151275167296\n",
      "STEP:  375\n",
      "loss: 0.05545659575887311\n",
      "test loss: 0.05362525335978061\n",
      "STEP:  376\n",
      "loss: 0.05545623220619561\n",
      "test loss: 0.05362436714313861\n",
      "STEP:  377\n",
      "loss: 0.05545586953717937\n",
      "test loss: 0.053623492419470696\n",
      "STEP:  378\n",
      "loss: 0.055455507741738096\n",
      "test loss: 0.05362262898675068\n",
      "STEP:  379\n",
      "loss: 0.055455146810065865\n",
      "test loss: 0.05362177664662691\n",
      "STEP:  380\n",
      "loss: 0.0554547867326378\n",
      "test loss: 0.0536209352043559\n",
      "STEP:  381\n",
      "loss: 0.05545442750020934\n",
      "test loss: 0.05362010446873824\n",
      "STEP:  382\n",
      "loss: 0.05545406910376989\n",
      "test loss: 0.053619284252052514\n",
      "STEP:  383\n",
      "loss: 0.05545371153459226\n",
      "test loss: 0.053618474369993806\n",
      "STEP:  384\n",
      "loss: 0.05545335478417879\n",
      "test loss: 0.053617674641612184\n",
      "STEP:  385\n",
      "loss: 0.055452998844269595\n",
      "test loss: 0.05361688488924972\n",
      "STEP:  386\n",
      "loss: 0.055452643706835635\n",
      "test loss: 0.05361610493848397\n",
      "STEP:  387\n",
      "loss: 0.0554522893640694\n",
      "test loss: 0.05361533461806644\n",
      "STEP:  388\n",
      "loss: 0.05545193580838184\n",
      "test loss: 0.05361457375986659\n",
      "STEP:  389\n",
      "loss: 0.05545158303239139\n",
      "test loss: 0.05361382219881633\n",
      "STEP:  390\n",
      "loss: 0.05545123102891956\n",
      "test loss: 0.053613079772853144\n",
      "STEP:  391\n",
      "loss: 0.05545087979097779\n",
      "test loss: 0.053612346322865906\n",
      "STEP:  392\n",
      "loss: 0.055450529311774015\n",
      "test loss: 0.053611621692640714\n",
      "STEP:  393\n",
      "loss: 0.05545017958469645\n",
      "test loss: 0.05361090572881172\n",
      "STEP:  394\n",
      "loss: 0.055449830603315264\n",
      "test loss: 0.05361019828080553\n",
      "STEP:  395\n",
      "loss: 0.055449482361364516\n",
      "test loss: 0.05360949920079289\n",
      "STEP:  396\n",
      "loss: 0.05544913485276276\n",
      "test loss: 0.053608808343639375\n",
      "STEP:  397\n",
      "loss: 0.05544878807157097\n",
      "test loss: 0.05360812556685426\n",
      "STEP:  398\n",
      "loss: 0.055448442012016944\n",
      "test loss: 0.05360745073054617\n",
      "STEP:  399\n",
      "loss: 0.05544809666847986\n",
      "test loss: 0.05360678369737287\n",
      "STEP:  400\n",
      "loss: 0.05544775203548559\n",
      "test loss: 0.05360612433249627\n",
      "STEP:  401\n",
      "loss: 0.05544740810770341\n",
      "test loss: 0.0536054725035388\n",
      "STEP:  402\n",
      "loss: 0.055447064879943844\n",
      "test loss: 0.05360482808053523\n",
      "STEP:  403\n",
      "loss: 0.055446722347142055\n",
      "test loss: 0.053604190935892894\n",
      "STEP:  404\n",
      "loss: 0.05544638050437636\n",
      "test loss: 0.05360356094434565\n",
      "STEP:  405\n",
      "loss: 0.05544603934684797\n",
      "test loss: 0.053602937982914\n",
      "STEP:  406\n",
      "loss: 0.05544569886986834\n",
      "test loss: 0.053602321930862395\n",
      "STEP:  407\n",
      "loss: 0.05544535906888029\n",
      "test loss: 0.053601712669658574\n",
      "STEP:  408\n",
      "loss: 0.055445019939448975\n",
      "test loss: 0.05360111008293506\n",
      "STEP:  409\n",
      "loss: 0.05544468147723083\n",
      "test loss: 0.053600514056448285\n",
      "STEP:  410\n",
      "loss: 0.055444343678000114\n",
      "test loss: 0.05359992447804035\n",
      "STEP:  411\n",
      "loss: 0.05544400653764625\n",
      "test loss: 0.053599341237603154\n",
      "STEP:  412\n",
      "loss: 0.05544367005214427\n",
      "test loss: 0.05359876422703852\n",
      "STEP:  413\n",
      "loss: 0.055443334217569744\n",
      "test loss: 0.053598193340223234\n",
      "STEP:  414\n",
      "loss: 0.055442999030108374\n",
      "test loss: 0.053597628472973134\n",
      "STEP:  415\n",
      "loss: 0.05544266448602659\n",
      "test loss: 0.05359706952300912\n",
      "STEP:  416\n",
      "loss: 0.05544233058168368\n",
      "test loss: 0.05359651638991976\n",
      "STEP:  417\n",
      "loss: 0.05544199731352653\n",
      "test loss: 0.05359596897512937\n",
      "STEP:  418\n",
      "loss: 0.05544166467808892\n",
      "test loss: 0.053595427181866204\n",
      "STEP:  419\n",
      "loss: 0.05544133267198209\n",
      "test loss: 0.05359489091512496\n",
      "STEP:  420\n",
      "loss: 0.055441001291908196\n",
      "test loss: 0.05359436008164059\n",
      "STEP:  421\n",
      "loss: 0.05544067053463081\n",
      "test loss: 0.053593834589852604\n",
      "STEP:  422\n",
      "loss: 0.05544034039699618\n",
      "test loss: 0.05359331434987552\n",
      "STEP:  423\n",
      "loss: 0.05544001087593435\n",
      "test loss: 0.05359279927346874\n",
      "STEP:  424\n",
      "loss: 0.05543968196843391\n",
      "test loss: 0.05359228927400585\n",
      "STEP:  425\n",
      "loss: 0.05543935367154916\n",
      "test loss: 0.05359178426644617\n",
      "STEP:  426\n",
      "loss: 0.05543902598241907\n",
      "test loss: 0.05359128416730561\n",
      "STEP:  427\n",
      "loss: 0.05543869889822601\n",
      "test loss: 0.05359078889462796\n",
      "STEP:  428\n",
      "loss: 0.055438372416230605\n",
      "test loss: 0.053590298367958644\n",
      "STEP:  429\n",
      "loss: 0.05543804653374957\n",
      "test loss: 0.053589812508314835\n",
      "STEP:  430\n",
      "loss: 0.055437721248157594\n",
      "test loss: 0.053589331238161196\n",
      "STEP:  431\n",
      "loss: 0.055437396556889826\n",
      "test loss: 0.053588854481382994\n",
      "STEP:  432\n",
      "loss: 0.05543707245743599\n",
      "test loss: 0.053588382163258855\n",
      "STEP:  433\n",
      "loss: 0.055436748947340415\n",
      "test loss: 0.05358791421043633\n",
      "STEP:  434\n",
      "loss: 0.055436426024203636\n",
      "test loss: 0.05358745055090844\n",
      "STEP:  435\n",
      "loss: 0.05543610368567338\n",
      "test loss: 0.053586991113986925\n",
      "STEP:  436\n",
      "loss: 0.05543578192944533\n",
      "test loss: 0.05358653583027853\n",
      "STEP:  437\n",
      "loss: 0.055435460753277974\n",
      "test loss: 0.053586084631664054\n",
      "STEP:  438\n",
      "loss: 0.055435140154950234\n",
      "test loss: 0.053585637451270804\n",
      "STEP:  439\n",
      "loss: 0.055434820132315696\n",
      "test loss: 0.05358519422345422\n",
      "STEP:  440\n",
      "loss: 0.05543450068325287\n",
      "test loss: 0.053584754883772276\n",
      "STEP:  441\n",
      "loss: 0.05543418180569445\n",
      "test loss: 0.05358431936896545\n",
      "STEP:  442\n",
      "loss: 0.055433863497607955\n",
      "test loss: 0.053583887616934084\n",
      "STEP:  443\n",
      "loss: 0.05543354575700649\n",
      "test loss: 0.053583459566718325\n",
      "STEP:  444\n",
      "loss: 0.05543322858194457\n",
      "test loss: 0.05358303515847557\n",
      "STEP:  445\n",
      "loss: 0.0554329119705054\n",
      "test loss: 0.05358261433346298\n",
      "STEP:  446\n",
      "loss: 0.05543259592082354\n",
      "test loss: 0.053582197034014116\n",
      "STEP:  447\n",
      "loss: 0.05543228043105868\n",
      "test loss: 0.05358178320352151\n",
      "STEP:  448\n",
      "loss: 0.05543196549941277\n",
      "test loss: 0.0535813727864165\n",
      "STEP:  449\n",
      "loss: 0.055431651124119956\n",
      "test loss: 0.053580965728150194\n",
      "STEP:  450\n",
      "loss: 0.05543133730344084\n",
      "test loss: 0.05358056197517544\n",
      "STEP:  451\n",
      "loss: 0.05543102403568945\n",
      "test loss: 0.05358016147492759\n",
      "STEP:  452\n",
      "loss: 0.055430711319185394\n",
      "test loss: 0.053579764175807926\n",
      "STEP:  453\n",
      "loss: 0.05543039915230029\n",
      "test loss: 0.05357937002716394\n",
      "STEP:  454\n",
      "loss: 0.05543008753342314\n",
      "test loss: 0.053578978979274446\n",
      "STEP:  455\n",
      "loss: 0.05542977646097378\n",
      "test loss: 0.05357859098333038\n",
      "STEP:  456\n",
      "loss: 0.05542946593341462\n",
      "test loss: 0.05357820599141989\n",
      "STEP:  457\n",
      "loss: 0.05542915594920714\n",
      "test loss: 0.05357782395651003\n",
      "STEP:  458\n",
      "loss: 0.05542884650687618\n",
      "test loss: 0.0535774448324323\n",
      "STEP:  459\n",
      "loss: 0.055428537604935095\n",
      "test loss: 0.053577068573866723\n",
      "STEP:  460\n",
      "loss: 0.05542822924195375\n",
      "test loss: 0.0535766951363247\n",
      "STEP:  461\n",
      "loss: 0.05542792141651176\n",
      "test loss: 0.05357632447613545\n",
      "STEP:  462\n",
      "loss: 0.0554276141272103\n",
      "test loss: 0.053575956550430605\n",
      "STEP:  463\n",
      "loss: 0.05542730737268203\n",
      "test loss: 0.053575591317128496\n",
      "STEP:  464\n",
      "loss: 0.05542700115157979\n",
      "test loss: 0.05357522873492141\n",
      "STEP:  465\n",
      "loss: 0.055426695462579646\n",
      "test loss: 0.05357486876326026\n",
      "STEP:  466\n",
      "loss: 0.05542639030437798\n",
      "test loss: 0.0535745113623396\n",
      "STEP:  467\n",
      "loss: 0.055426085675689026\n",
      "test loss: 0.053574156493087416\n",
      "STEP:  468\n",
      "loss: 0.055425781575253366\n",
      "test loss: 0.05357380411714786\n",
      "STEP:  469\n",
      "loss: 0.055425478001827296\n",
      "test loss: 0.05357345419687003\n",
      "STEP:  470\n",
      "loss: 0.05542517495419144\n",
      "test loss: 0.05357310669529534\n",
      "STEP:  471\n",
      "loss: 0.05542487243114425\n",
      "test loss: 0.053572761576143206\n",
      "STEP:  472\n",
      "loss: 0.055424570431493424\n",
      "test loss: 0.05357241880379966\n",
      "STEP:  473\n",
      "loss: 0.05542426895407852\n",
      "test loss: 0.05357207834330528\n",
      "STEP:  474\n",
      "loss: 0.05542396799774523\n",
      "test loss: 0.05357174016034264\n",
      "STEP:  475\n",
      "loss: 0.055423667561366415\n",
      "test loss: 0.05357140422122446\n",
      "STEP:  476\n",
      "loss: 0.05542336764382097\n",
      "test loss: 0.05357107049288175\n",
      "STEP:  477\n",
      "loss: 0.05542306824400972\n",
      "test loss: 0.053570738942853774\n",
      "STEP:  478\n",
      "loss: 0.05542276936085132\n",
      "test loss: 0.05357040953927422\n",
      "STEP:  479\n",
      "loss: 0.05542247099328136\n",
      "test loss: 0.0535700822508639\n",
      "STEP:  480\n",
      "loss: 0.055422173140237625\n",
      "test loss: 0.053569757046915675\n",
      "STEP:  481\n",
      "loss: 0.05542187580068202\n",
      "test loss: 0.05356943389728799\n",
      "STEP:  482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.05542157897359468\n",
      "test loss: 0.053569112772391174\n",
      "STEP:  483\n",
      "loss: 0.0554212826579598\n",
      "test loss: 0.05356879364317892\n",
      "STEP:  484\n",
      "loss: 0.05542098685278536\n",
      "test loss: 0.05356847648113833\n",
      "STEP:  485\n",
      "loss: 0.05542069155708126\n",
      "test loss: 0.05356816125827951\n",
      "STEP:  486\n",
      "loss: 0.0554203967698766\n",
      "test loss: 0.05356784794712545\n",
      "STEP:  487\n",
      "loss: 0.05542010249021559\n",
      "test loss: 0.053567536520703035\n",
      "STEP:  488\n",
      "loss: 0.05541980871714673\n",
      "test loss: 0.05356722695253411\n",
      "STEP:  489\n",
      "loss: 0.055419515449738006\n",
      "test loss: 0.05356691921662622\n",
      "STEP:  490\n",
      "loss: 0.05541922268706541\n",
      "test loss: 0.05356661328746314\n",
      "STEP:  491\n",
      "loss: 0.055418930428212256\n",
      "test loss: 0.05356630913999539\n",
      "STEP:  492\n",
      "loss: 0.0554186386722836\n",
      "test loss: 0.05356600674963378\n",
      "STEP:  493\n",
      "loss: 0.055418347418382224\n",
      "test loss: 0.053565706092238104\n",
      "STEP:  494\n",
      "loss: 0.055418056665630426\n",
      "test loss: 0.05356540714411162\n",
      "STEP:  495\n",
      "loss: 0.05541776641315446\n",
      "test loss: 0.05356510988199082\n",
      "STEP:  496\n",
      "loss: 0.05541747666009486\n",
      "test loss: 0.05356481428303684\n",
      "STEP:  497\n",
      "loss: 0.055417187405603224\n",
      "test loss: 0.05356452032483044\n",
      "STEP:  498\n",
      "loss: 0.055416898648842035\n",
      "test loss: 0.05356422798536162\n",
      "STEP:  499\n",
      "loss: 0.05541661038896536\n",
      "test loss: 0.05356393724302234\n",
      "STEP:  500\n",
      "loss: 0.05541632262516039\n",
      "test loss: 0.053563648076600814\n",
      "STEP:  501\n",
      "loss: 0.05541603535660721\n",
      "test loss: 0.05356336046527111\n",
      "STEP:  502\n",
      "loss: 0.0554157485825012\n",
      "test loss: 0.053563074388589677\n",
      "STEP:  503\n",
      "loss: 0.055415462302045734\n",
      "test loss: 0.05356278982648468\n",
      "STEP:  504\n",
      "loss: 0.05541517651444532\n",
      "test loss: 0.05356250675925093\n",
      "STEP:  505\n",
      "loss: 0.05541489121892211\n",
      "test loss: 0.05356222516754275\n",
      "STEP:  506\n",
      "loss: 0.055414606414695006\n",
      "test loss: 0.053561945032368455\n",
      "STEP:  507\n",
      "loss: 0.05541432210100659\n",
      "test loss: 0.05356166633508032\n",
      "STEP:  508\n",
      "loss: 0.05541403827708774\n",
      "test loss: 0.053561389057371976\n",
      "STEP:  509\n",
      "loss: 0.05541375494218549\n",
      "test loss: 0.0535611131812694\n",
      "STEP:  510\n",
      "loss: 0.05541347209555863\n",
      "test loss: 0.05356083868912692\n",
      "STEP:  511\n",
      "loss: 0.05541318973646619\n",
      "test loss: 0.05356056556361923\n",
      "STEP:  512\n",
      "loss: 0.05541290786417209\n",
      "test loss: 0.053560293787735455\n",
      "STEP:  513\n",
      "loss: 0.055412626477950554\n",
      "test loss: 0.05356002334477469\n",
      "STEP:  514\n",
      "loss: 0.05541234557708326\n",
      "test loss: 0.05355975421833869\n",
      "STEP:  515\n",
      "loss: 0.055412065160850725\n",
      "test loss: 0.05355948639232708\n",
      "STEP:  516\n",
      "loss: 0.055411785228553063\n",
      "test loss: 0.053559219850931726\n",
      "STEP:  517\n",
      "loss: 0.05541150577947893\n",
      "test loss: 0.053558954578630684\n",
      "STEP:  518\n",
      "loss: 0.055411226812935234\n",
      "test loss: 0.05355869056018383\n",
      "STEP:  519\n",
      "loss: 0.05541094832822521\n",
      "test loss: 0.05355842778062586\n",
      "STEP:  520\n",
      "loss: 0.0554106703246654\n",
      "test loss: 0.05355816622526319\n",
      "STEP:  521\n",
      "loss: 0.05541039280158011\n",
      "test loss: 0.053557905879667383\n",
      "STEP:  522\n",
      "loss: 0.055410115758283404\n",
      "test loss: 0.053557646729670984\n",
      "STEP:  523\n",
      "loss: 0.055409839194107455\n",
      "test loss: 0.053557388761361215\n",
      "STEP:  524\n",
      "loss: 0.055409563108386205\n",
      "test loss: 0.053557131961077406\n",
      "STEP:  525\n",
      "loss: 0.055409287500460196\n",
      "test loss: 0.05355687631540468\n",
      "STEP:  526\n",
      "loss: 0.055409012369662725\n",
      "test loss: 0.053556621811169186\n",
      "STEP:  527\n",
      "loss: 0.05540873771534961\n",
      "test loss: 0.05355636843543507\n",
      "STEP:  528\n",
      "loss: 0.05540846353686774\n",
      "test loss: 0.05355611617549782\n",
      "STEP:  529\n",
      "loss: 0.055408189833575046\n",
      "test loss: 0.053555865018881516\n",
      "STEP:  530\n",
      "loss: 0.055407916604829716\n",
      "test loss: 0.05355561495333472\n",
      "STEP:  531\n",
      "loss: 0.05540764384998853\n",
      "test loss: 0.05355536596682462\n",
      "STEP:  532\n",
      "loss: 0.055407371568428884\n",
      "test loss: 0.05355511804753395\n",
      "STEP:  533\n",
      "loss: 0.05540709975951978\n",
      "test loss: 0.05355487118385731\n",
      "STEP:  534\n",
      "loss: 0.055406828422628736\n",
      "test loss: 0.05355462536439562\n",
      "STEP:  535\n",
      "loss: 0.055406557557141085\n",
      "test loss: 0.05355438057795477\n",
      "STEP:  536\n",
      "loss: 0.05540628716243441\n",
      "test loss: 0.05355413681353845\n",
      "STEP:  537\n",
      "loss: 0.05540601723789869\n",
      "test loss: 0.0535538940603485\n",
      "STEP:  538\n",
      "loss: 0.0554057477829166\n",
      "test loss: 0.053553652307775715\n",
      "STEP:  539\n",
      "loss: 0.055405478796881104\n",
      "test loss: 0.053553411545401296\n",
      "STEP:  540\n",
      "loss: 0.05540521027918549\n",
      "test loss: 0.05355317176299155\n",
      "STEP:  541\n",
      "loss: 0.05540494222923091\n",
      "test loss: 0.053552932950492974\n",
      "STEP:  542\n",
      "loss: 0.05540467464641199\n",
      "test loss: 0.05355269509803045\n",
      "STEP:  543\n",
      "loss: 0.05540440753013753\n",
      "test loss: 0.05355245819590286\n",
      "STEP:  544\n",
      "loss: 0.055404140879815315\n",
      "test loss: 0.05355222223458154\n",
      "STEP:  545\n",
      "loss: 0.05540387469484749\n",
      "test loss: 0.0535519872047044\n",
      "STEP:  546\n",
      "loss: 0.05540360897464762\n",
      "test loss: 0.053551753097074446\n",
      "STEP:  547\n",
      "loss: 0.05540334371863148\n",
      "test loss: 0.053551519902656795\n",
      "STEP:  548\n",
      "loss: 0.05540307892621468\n",
      "test loss: 0.05355128761257399\n",
      "STEP:  549\n",
      "loss: 0.05540281459681788\n",
      "test loss: 0.053551056218105034\n",
      "STEP:  550\n",
      "loss: 0.055402550729854436\n",
      "test loss: 0.053550825710680684\n",
      "STEP:  551\n",
      "loss: 0.05540228732475687\n",
      "test loss: 0.053550596081882146\n",
      "STEP:  552\n",
      "loss: 0.05540202438094645\n",
      "test loss: 0.05355036732343638\n",
      "STEP:  553\n",
      "loss: 0.055401761897848845\n",
      "test loss: 0.05355013942721418\n",
      "STEP:  554\n",
      "loss: 0.05540149987490184\n",
      "test loss: 0.05354991238522854\n",
      "STEP:  555\n",
      "loss: 0.055401238311527684\n",
      "test loss: 0.053549686189630304\n",
      "STEP:  556\n",
      "loss: 0.05540097720716748\n",
      "test loss: 0.053549460832706064\n",
      "STEP:  557\n",
      "loss: 0.05540071656124928\n",
      "test loss: 0.05354923630687518\n",
      "STEP:  558\n",
      "loss: 0.05540045637321996\n",
      "test loss: 0.05354901260468841\n",
      "STEP:  559\n",
      "loss: 0.05540019664250923\n",
      "test loss: 0.053548789718824395\n",
      "STEP:  560\n",
      "loss: 0.05539993736856314\n",
      "test loss: 0.053548567642087125\n",
      "STEP:  561\n",
      "loss: 0.055399678550824345\n",
      "test loss: 0.0535483463674039\n",
      "STEP:  562\n",
      "loss: 0.05539942018873753\n",
      "test loss: 0.05354812588782359\n",
      "STEP:  563\n",
      "loss: 0.05539916228174555\n",
      "test loss: 0.05354790619651232\n",
      "STEP:  564\n",
      "loss: 0.05539890482930254\n",
      "test loss: 0.053547687286754166\n",
      "STEP:  565\n",
      "loss: 0.05539864783084429\n",
      "test loss: 0.053547469151944645\n",
      "STEP:  566\n",
      "loss: 0.05539839128583102\n",
      "test loss: 0.05354725178559414\n",
      "STEP:  567\n",
      "loss: 0.05539813519371397\n",
      "test loss: 0.05354703518132062\n",
      "STEP:  568\n",
      "loss: 0.05539787955394165\n",
      "test loss: 0.05354681933284983\n",
      "STEP:  569\n",
      "loss: 0.055397624365973626\n",
      "test loss: 0.05354660423401413\n",
      "STEP:  570\n",
      "loss: 0.05539736962925704\n",
      "test loss: 0.05354638987874773\n",
      "STEP:  571\n",
      "loss: 0.055397115343254436\n",
      "test loss: 0.053546176261087756\n",
      "STEP:  572\n",
      "loss: 0.05539686150742964\n",
      "test loss: 0.05354596337516952\n",
      "STEP:  573\n",
      "loss: 0.055396608121221776\n",
      "test loss: 0.053545751215226825\n",
      "STEP:  574\n",
      "loss: 0.05539635518411239\n",
      "test loss: 0.053545539775587916\n",
      "STEP:  575\n",
      "loss: 0.05539610269554848\n",
      "test loss: 0.05354532905067652\n",
      "STEP:  576\n",
      "loss: 0.05539585065499935\n",
      "test loss: 0.05354511903500712\n",
      "STEP:  577\n",
      "loss: 0.05539559906192235\n",
      "test loss: 0.05354490972318375\n",
      "STEP:  578\n",
      "loss: 0.05539534791578711\n",
      "test loss: 0.05354470110990082\n",
      "STEP:  579\n",
      "loss: 0.05539509721605272\n",
      "test loss: 0.05354449318993809\n",
      "STEP:  580\n",
      "loss: 0.05539484696218773\n",
      "test loss: 0.053544285958160735\n",
      "STEP:  581\n",
      "loss: 0.05539459715365691\n",
      "test loss: 0.05354407940951698\n",
      "STEP:  582\n",
      "loss: 0.055394347789929056\n",
      "test loss: 0.053543873539036835\n",
      "STEP:  583\n",
      "loss: 0.05539409887046779\n",
      "test loss: 0.05354366834183091\n",
      "STEP:  584\n",
      "loss: 0.05539385039474506\n",
      "test loss: 0.05354346381308764\n",
      "STEP:  585\n",
      "loss: 0.055393602362231835\n",
      "test loss: 0.05354325994807321\n",
      "STEP:  586\n",
      "loss: 0.05539335477239553\n",
      "test loss: 0.05354305674212891\n",
      "STEP:  587\n",
      "loss: 0.05539310762470242\n",
      "test loss: 0.05354285419067028\n",
      "STEP:  588\n",
      "loss: 0.0553928609186329\n",
      "test loss: 0.053542652289185255\n",
      "STEP:  589\n",
      "loss: 0.05539261465364881\n",
      "test loss: 0.05354245103323343\n",
      "STEP:  590\n",
      "loss: 0.05539236882923106\n",
      "test loss: 0.053542250418443856\n",
      "STEP:  591\n",
      "loss: 0.055392123444844184\n",
      "test loss: 0.053542050440513585\n",
      "STEP:  592\n",
      "loss: 0.05539187849996747\n",
      "test loss: 0.05354185109520755\n",
      "STEP:  593\n",
      "loss: 0.05539163399406868\n",
      "test loss: 0.05354165237835575\n",
      "STEP:  594\n",
      "loss: 0.05539138992662816\n",
      "test loss: 0.05354145428585325\n",
      "STEP:  595\n",
      "loss: 0.055391146297113024\n",
      "test loss: 0.0535412568136577\n",
      "STEP:  596\n",
      "loss: 0.05539090310500534\n",
      "test loss: 0.05354105995778948\n",
      "STEP:  597\n",
      "loss: 0.055390660349776216\n",
      "test loss: 0.05354086371432884\n",
      "STEP:  598\n",
      "loss: 0.05539041803090046\n",
      "test loss: 0.05354066807941574\n",
      "STEP:  599\n",
      "loss: 0.05539017614785649\n",
      "test loss: 0.053540473049249845\n",
      "STEP:  600\n",
      "loss: 0.055389934700116286\n",
      "test loss: 0.05354027862008574\n",
      "STEP:  601\n",
      "loss: 0.055389693687164024\n",
      "test loss: 0.053540084788236075\n",
      "STEP:  602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.05538945310846721\n",
      "test loss: 0.05353989155006734\n",
      "STEP:  603\n",
      "loss: 0.05538921296350576\n",
      "test loss: 0.05353969890200032\n",
      "STEP:  604\n",
      "loss: 0.055388973251756804\n",
      "test loss: 0.05353950684050862\n",
      "STEP:  605\n",
      "loss: 0.05538873397270173\n",
      "test loss: 0.05353931536211729\n",
      "STEP:  606\n",
      "loss: 0.05538849512580858\n",
      "test loss: 0.053539124463402865\n",
      "STEP:  607\n",
      "loss: 0.055388256710562446\n",
      "test loss: 0.05353893414099089\n",
      "STEP:  608\n",
      "loss: 0.05538801872643841\n",
      "test loss: 0.05353874439155619\n",
      "STEP:  609\n",
      "loss: 0.055387781172913486\n",
      "test loss: 0.05353855521182067\n",
      "STEP:  610\n",
      "loss: 0.0553875440494653\n",
      "test loss: 0.05353836659855391\n",
      "STEP:  611\n",
      "loss: 0.05538730735557225\n",
      "test loss: 0.05353817854857046\n",
      "STEP:  612\n",
      "loss: 0.0553870710907126\n",
      "test loss: 0.05353799105873118\n",
      "STEP:  613\n",
      "loss: 0.05538683525436669\n",
      "test loss: 0.05353780412593987\n",
      "STEP:  614\n",
      "loss: 0.05538659984600709\n",
      "test loss: 0.05353761774714352\n",
      "STEP:  615\n",
      "loss: 0.05538636486511624\n",
      "test loss: 0.053537431919332204\n",
      "STEP:  616\n",
      "loss: 0.05538613031116542\n",
      "test loss: 0.05353724663953771\n",
      "STEP:  617\n",
      "loss: 0.05538589618363885\n",
      "test loss: 0.05353706190483126\n",
      "STEP:  618\n",
      "loss: 0.055385662482014986\n",
      "test loss: 0.05353687771232526\n",
      "STEP:  619\n",
      "loss: 0.055385429205769145\n",
      "test loss: 0.053536694059169804\n",
      "STEP:  620\n",
      "loss: 0.05538519635437728\n",
      "test loss: 0.053536510942554606\n",
      "STEP:  621\n",
      "loss: 0.05538496392732229\n",
      "test loss: 0.05353632835970646\n",
      "STEP:  622\n",
      "loss: 0.05538473192407369\n",
      "test loss: 0.05353614630788822\n",
      "STEP:  623\n",
      "loss: 0.0553845003441171\n",
      "test loss: 0.05353596478439998\n",
      "STEP:  624\n",
      "loss: 0.05538426918693053\n",
      "test loss: 0.05353578378657625\n",
      "STEP:  625\n",
      "loss: 0.055384038451981886\n",
      "test loss: 0.0535356033117863\n",
      "STEP:  626\n",
      "loss: 0.05538380813875349\n",
      "test loss: 0.0535354233574336\n",
      "STEP:  627\n",
      "loss: 0.055383578246724005\n",
      "test loss: 0.05353524392095497\n",
      "STEP:  628\n",
      "loss: 0.055383348775369746\n",
      "test loss: 0.05353506499981855\n",
      "STEP:  629\n",
      "loss: 0.055383119724166555\n",
      "test loss: 0.053534886591526366\n",
      "STEP:  630\n",
      "loss: 0.0553828910925887\n",
      "test loss: 0.05353470869360973\n",
      "STEP:  631\n",
      "loss: 0.05538266288011623\n",
      "test loss: 0.053534531303631755\n",
      "STEP:  632\n",
      "loss: 0.055382435086225076\n",
      "test loss: 0.053534354419185706\n",
      "STEP:  633\n",
      "loss: 0.05538220771038474\n",
      "test loss: 0.05353417803789316\n",
      "STEP:  634\n",
      "loss: 0.055381980752079715\n",
      "test loss: 0.053534002157405336\n",
      "STEP:  635\n",
      "loss: 0.05538175421078126\n",
      "test loss: 0.053533826775401944\n",
      "STEP:  636\n",
      "loss: 0.05538152808596135\n",
      "test loss: 0.05353365188958919\n",
      "STEP:  637\n",
      "loss: 0.05538130237709784\n",
      "test loss: 0.053533477497701466\n",
      "STEP:  638\n",
      "loss: 0.05538107708366765\n",
      "test loss: 0.05353330359749906\n",
      "STEP:  639\n",
      "loss: 0.055380852205146006\n",
      "test loss: 0.05353313018676885\n",
      "STEP:  640\n",
      "loss: 0.055380627740999014\n",
      "test loss: 0.05353295726332221\n",
      "STEP:  641\n",
      "loss: 0.05538040369071041\n",
      "test loss: 0.053532784824997084\n",
      "STEP:  642\n",
      "loss: 0.05538018005374549\n",
      "test loss: 0.05353261286965428\n",
      "STEP:  643\n",
      "loss: 0.05537995682958365\n",
      "test loss: 0.05353244139517871\n",
      "STEP:  644\n",
      "loss: 0.055379734017695526\n",
      "test loss: 0.053532270399479574\n",
      "STEP:  645\n",
      "loss: 0.05537951161755055\n",
      "test loss: 0.053532099880489535\n",
      "STEP:  646\n",
      "loss: 0.05537928962862598\n",
      "test loss: 0.053531929836161576\n",
      "STEP:  647\n",
      "loss: 0.0553790680503954\n",
      "test loss: 0.05353176026447243\n",
      "STEP:  648\n",
      "loss: 0.055378846882322905\n",
      "test loss: 0.05353159116342072\n",
      "STEP:  649\n",
      "loss: 0.055378626123884354\n",
      "test loss: 0.05353142253102502\n",
      "STEP:  650\n",
      "loss: 0.05537840577455347\n",
      "test loss: 0.05353125436532555\n",
      "STEP:  651\n",
      "loss: 0.055378185833794905\n",
      "test loss: 0.05353108666438209\n",
      "STEP:  652\n",
      "loss: 0.055377966301084075\n",
      "test loss: 0.0535309194262751\n",
      "STEP:  653\n",
      "loss: 0.05537774717588948\n",
      "test loss: 0.05353075264910458\n",
      "STEP:  654\n",
      "loss: 0.05537752845768069\n",
      "test loss: 0.053530586330987805\n",
      "STEP:  655\n",
      "loss: 0.05537731014592623\n",
      "test loss: 0.05353042047006368\n",
      "STEP:  656\n",
      "loss: 0.05537709224009698\n",
      "test loss: 0.05353025506448662\n",
      "STEP:  657\n",
      "loss: 0.05537687473966252\n",
      "test loss: 0.0535300901124316\n",
      "STEP:  658\n",
      "loss: 0.05537665764408384\n",
      "test loss: 0.053529925612089045\n",
      "STEP:  659\n",
      "loss: 0.05537644095283566\n",
      "test loss: 0.05352976156166715\n",
      "STEP:  660\n",
      "loss: 0.055376224665387414\n",
      "test loss: 0.053529597959392\n",
      "STEP:  661\n",
      "loss: 0.05537600878119907\n",
      "test loss: 0.053529434803504194\n",
      "STEP:  662\n",
      "loss: 0.055375793299738345\n",
      "test loss: 0.05352927209226285\n",
      "STEP:  663\n",
      "loss: 0.05537557822047645\n",
      "test loss: 0.05352910982394096\n",
      "STEP:  664\n",
      "loss: 0.05537536354287871\n",
      "test loss: 0.05352894799682848\n",
      "STEP:  665\n",
      "loss: 0.05537514926640625\n",
      "test loss: 0.05352878660922971\n",
      "STEP:  666\n",
      "loss: 0.05537493539052766\n",
      "test loss: 0.05352862565946379\n",
      "STEP:  667\n",
      "loss: 0.05537472191470813\n",
      "test loss: 0.05352846514586476\n",
      "STEP:  668\n",
      "loss: 0.055374508838407194\n",
      "test loss: 0.053528305066780836\n",
      "STEP:  669\n",
      "loss: 0.05537429616109603\n",
      "test loss: 0.05352814542057453\n",
      "STEP:  670\n",
      "loss: 0.0553740838822344\n",
      "test loss: 0.05352798620562207\n",
      "STEP:  671\n",
      "loss: 0.055373872001279946\n",
      "test loss: 0.053527827420311605\n",
      "STEP:  672\n",
      "loss: 0.05537366051770445\n",
      "test loss: 0.0535276690630466\n",
      "STEP:  673\n",
      "loss: 0.055373449430966615\n",
      "test loss: 0.053527511132241926\n",
      "STEP:  674\n",
      "loss: 0.05537323874052814\n",
      "test loss: 0.05352735362632564\n",
      "STEP:  675\n",
      "loss: 0.05537302844584858\n",
      "test loss: 0.05352719654373764\n",
      "STEP:  676\n",
      "loss: 0.05537281854639424\n",
      "test loss: 0.05352703988293066\n",
      "STEP:  677\n",
      "loss: 0.055372609041621834\n",
      "test loss: 0.05352688364236927\n",
      "STEP:  678\n",
      "loss: 0.0553723999309898\n",
      "test loss: 0.05352672782052853\n",
      "STEP:  679\n",
      "loss: 0.05537219121396215\n",
      "test loss: 0.05352657241589572\n",
      "STEP:  680\n",
      "loss: 0.05537198288999746\n",
      "test loss: 0.053526417426969175\n",
      "STEP:  681\n",
      "loss: 0.05537177495854946\n",
      "test loss: 0.05352626285225764\n",
      "STEP:  682\n",
      "loss: 0.055371567419084645\n",
      "test loss: 0.053526108690281814\n",
      "STEP:  683\n",
      "loss: 0.05537136027105462\n",
      "test loss: 0.053525954939570776\n",
      "STEP:  684\n",
      "loss: 0.05537115351392556\n",
      "test loss: 0.05352580159866562\n",
      "STEP:  685\n",
      "loss: 0.05537094714714124\n",
      "test loss: 0.05352564866611661\n",
      "STEP:  686\n",
      "loss: 0.05537074117017276\n",
      "test loss: 0.0535254961404841\n",
      "STEP:  687\n",
      "loss: 0.05537053558246521\n",
      "test loss: 0.053525344020337415\n",
      "STEP:  688\n",
      "loss: 0.05537033038348395\n",
      "test loss: 0.053525192304256876\n",
      "STEP:  689\n",
      "loss: 0.0553701255726762\n",
      "test loss: 0.05352504099082956\n",
      "STEP:  690\n",
      "loss: 0.055369921149503634\n",
      "test loss: 0.05352489007865459\n",
      "STEP:  691\n",
      "loss: 0.05536971711341434\n",
      "test loss: 0.05352473956633706\n",
      "STEP:  692\n",
      "loss: 0.05536951346386833\n",
      "test loss: 0.05352458945249215\n",
      "STEP:  693\n",
      "loss: 0.05536931020031912\n",
      "test loss: 0.053524439735743584\n",
      "STEP:  694\n",
      "loss: 0.05536910732221844\n",
      "test loss: 0.05352429041472374\n",
      "STEP:  695\n",
      "loss: 0.05536890482901781\n",
      "test loss: 0.053524141488071046\n",
      "STEP:  696\n",
      "loss: 0.05536870272017354\n",
      "test loss: 0.05352399295443421\n",
      "STEP:  697\n",
      "loss: 0.05536850099513372\n",
      "test loss: 0.053523844812469044\n",
      "STEP:  698\n",
      "loss: 0.05536829965335578\n",
      "test loss: 0.05352369706083835\n",
      "STEP:  699\n",
      "loss: 0.05536809869428401\n",
      "test loss: 0.05352354969821357\n",
      "STEP:  700\n",
      "loss: 0.055367898117376205\n",
      "test loss: 0.05352340272327231\n",
      "STEP:  701\n",
      "loss: 0.055367697922075375\n",
      "test loss: 0.05352325613469962\n",
      "STEP:  702\n",
      "loss: 0.05536749810784129\n",
      "test loss: 0.05352310993118861\n",
      "STEP:  703\n",
      "loss: 0.055367298674115384\n",
      "test loss: 0.05352296411143761\n",
      "STEP:  704\n",
      "loss: 0.055367099620349594\n",
      "test loss: 0.05352281867415351\n",
      "STEP:  705\n",
      "loss: 0.05536690094599396\n",
      "test loss: 0.05352267361804828\n",
      "STEP:  706\n",
      "loss: 0.05536670265049479\n",
      "test loss: 0.05352252894184056\n",
      "STEP:  707\n",
      "loss: 0.05536650473330154\n",
      "test loss: 0.053522384644256696\n",
      "STEP:  708\n",
      "loss: 0.05536630719386207\n",
      "test loss: 0.0535222407240275\n",
      "STEP:  709\n",
      "loss: 0.055366110031626885\n",
      "test loss: 0.05352209717989123\n",
      "STEP:  710\n",
      "loss: 0.05536591324603587\n",
      "test loss: 0.053521954010591156\n",
      "STEP:  711\n",
      "loss: 0.05536571683653829\n",
      "test loss: 0.05352181121487724\n",
      "STEP:  712\n",
      "loss: 0.055365520802582384\n",
      "test loss: 0.053521668791504204\n",
      "STEP:  713\n",
      "loss: 0.05536532514361135\n",
      "test loss: 0.05352152673923258\n",
      "STEP:  714\n",
      "loss: 0.055365129859072014\n",
      "test loss: 0.053521385056829324\n",
      "STEP:  715\n",
      "loss: 0.05536493494841227\n",
      "test loss: 0.053521243743066396\n",
      "STEP:  716\n",
      "loss: 0.055364740411067453\n",
      "test loss: 0.05352110279671951\n",
      "STEP:  717\n",
      "loss: 0.055364546246490884\n",
      "test loss: 0.05352096221657244\n",
      "STEP:  718\n",
      "loss: 0.05536435245412184\n",
      "test loss: 0.05352082200141191\n",
      "STEP:  719\n",
      "loss: 0.05536415903340349\n",
      "test loss: 0.053520682150029364\n",
      "STEP:  720\n",
      "loss: 0.05536396598378245\n",
      "test loss: 0.05352054266122238\n",
      "STEP:  721\n",
      "loss: 0.055363773304700076\n",
      "test loss: 0.05352040353379354\n",
      "STEP:  722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.055363580995591535\n",
      "test loss: 0.05352026476654839\n",
      "STEP:  723\n",
      "loss: 0.055363389055911254\n",
      "test loss: 0.053520126358298344\n",
      "STEP:  724\n",
      "loss: 0.05536319748509096\n",
      "test loss: 0.05351998830785889\n",
      "STEP:  725\n",
      "loss: 0.05536300628256996\n",
      "test loss: 0.05351985061405041\n",
      "STEP:  726\n",
      "loss: 0.0553628154478015\n",
      "test loss: 0.05351971327569745\n",
      "STEP:  727\n",
      "loss: 0.05536262498021505\n",
      "test loss: 0.05351957629162793\n",
      "STEP:  728\n",
      "loss: 0.05536243487925477\n",
      "test loss: 0.05351943966067509\n",
      "STEP:  729\n",
      "loss: 0.05536224514436039\n",
      "test loss: 0.05351930338167553\n",
      "STEP:  730\n",
      "loss: 0.05536205577496942\n",
      "test loss: 0.053519167453470365\n",
      "STEP:  731\n",
      "loss: 0.05536186677052056\n",
      "test loss: 0.05351903187490399\n",
      "STEP:  732\n",
      "loss: 0.05536167813045398\n",
      "test loss: 0.05351889664482503\n",
      "STEP:  733\n",
      "loss: 0.055361489854211195\n",
      "test loss: 0.05351876176208645\n",
      "STEP:  734\n",
      "loss: 0.05536130194122651\n",
      "test loss: 0.05351862722554396\n",
      "STEP:  735\n",
      "loss: 0.055361114390935606\n",
      "test loss: 0.05351849303405718\n",
      "STEP:  736\n",
      "loss: 0.05536092720277847\n",
      "test loss: 0.05351835918648956\n",
      "STEP:  737\n",
      "loss: 0.05536074037619373\n",
      "test loss: 0.05351822568170787\n",
      "STEP:  738\n",
      "loss: 0.055360553910616436\n",
      "test loss: 0.0535180925185819\n",
      "STEP:  739\n",
      "loss: 0.05536036780547707\n",
      "test loss: 0.05351795969598565\n",
      "STEP:  740\n",
      "loss: 0.055360182060224424\n",
      "test loss: 0.053517827212796076\n",
      "STEP:  741\n",
      "loss: 0.055359996674280126\n",
      "test loss: 0.053517695067893785\n",
      "STEP:  742\n",
      "loss: 0.05535981164709125\n",
      "test loss: 0.053517563260160494\n",
      "STEP:  743\n",
      "loss: 0.05535962697808658\n",
      "test loss: 0.0535174317884844\n",
      "STEP:  744\n",
      "loss: 0.05535944266670367\n",
      "test loss: 0.05351730065175454\n",
      "STEP:  745\n",
      "loss: 0.05535925871237736\n",
      "test loss: 0.05351716984886289\n",
      "STEP:  746\n",
      "loss: 0.05535907511453441\n",
      "test loss: 0.05351703937870582\n",
      "STEP:  747\n",
      "loss: 0.05535889187261891\n",
      "test loss: 0.053516909240181\n",
      "STEP:  748\n",
      "loss: 0.05535870898605757\n",
      "test loss: 0.0535167794321904\n",
      "STEP:  749\n",
      "loss: 0.05535852645429136\n",
      "test loss: 0.05351664995363806\n",
      "STEP:  750\n",
      "loss: 0.055358344276741786\n",
      "test loss: 0.053516520803430376\n",
      "STEP:  751\n",
      "loss: 0.05535816245285039\n",
      "test loss: 0.05351639198047794\n",
      "STEP:  752\n",
      "loss: 0.05535798098204876\n",
      "test loss: 0.0535162634836918\n",
      "STEP:  753\n",
      "loss: 0.055357799863765586\n",
      "test loss: 0.053516135311987935\n",
      "STEP:  754\n",
      "loss: 0.05535761909743828\n",
      "test loss: 0.05351600746428317\n",
      "STEP:  755\n",
      "loss: 0.05535743868249353\n",
      "test loss: 0.05351587993949826\n",
      "STEP:  756\n",
      "loss: 0.05535725861836036\n",
      "test loss: 0.05351575273655501\n",
      "STEP:  757\n",
      "loss: 0.05535707890448076\n",
      "test loss: 0.0535156258543791\n",
      "STEP:  758\n",
      "loss: 0.05535689954027519\n",
      "test loss: 0.053515499291897244\n",
      "STEP:  759\n",
      "loss: 0.055356720525176574\n",
      "test loss: 0.05351537304804016\n",
      "STEP:  760\n",
      "loss: 0.05535654185862071\n",
      "test loss: 0.05351524712173907\n",
      "STEP:  761\n",
      "loss: 0.05535636354002895\n",
      "test loss: 0.05351512151192914\n",
      "STEP:  762\n",
      "loss: 0.055356185568843384\n",
      "test loss: 0.053514996217547114\n",
      "STEP:  763\n",
      "loss: 0.055356007944480874\n",
      "test loss: 0.05351487123753189\n",
      "STEP:  764\n",
      "loss: 0.05535583066637758\n",
      "test loss: 0.05351474657082437\n",
      "STEP:  765\n",
      "loss: 0.05535565373396355\n",
      "test loss: 0.05351462221636841\n",
      "STEP:  766\n",
      "loss: 0.05535547714666462\n",
      "test loss: 0.05351449817310905\n",
      "STEP:  767\n",
      "loss: 0.05535530090390952\n",
      "test loss: 0.05351437443999397\n",
      "STEP:  768\n",
      "loss: 0.05535512500513304\n",
      "test loss: 0.05351425101597366\n",
      "STEP:  769\n",
      "loss: 0.05535494944975689\n",
      "test loss: 0.053514127899999105\n",
      "STEP:  770\n",
      "loss: 0.055354774237208054\n",
      "test loss: 0.05351400509102447\n",
      "STEP:  771\n",
      "loss: 0.055354599366927035\n",
      "test loss: 0.05351388258800535\n",
      "STEP:  772\n",
      "loss: 0.0553544248383255\n",
      "test loss: 0.05351376038989957\n",
      "STEP:  773\n",
      "loss: 0.055354250650837905\n",
      "test loss: 0.05351363849566681\n",
      "STEP:  774\n",
      "loss: 0.05535407680389441\n",
      "test loss: 0.053513516904268886\n",
      "STEP:  775\n",
      "loss: 0.05535390329691898\n",
      "test loss: 0.053513395614669196\n",
      "STEP:  776\n",
      "loss: 0.05535373012934091\n",
      "test loss: 0.05351327462583313\n",
      "STEP:  777\n",
      "loss: 0.05535355730058862\n",
      "test loss: 0.05351315393672799\n",
      "STEP:  778\n",
      "loss: 0.05535338481008208\n",
      "test loss: 0.053513033546322586\n",
      "STEP:  779\n",
      "loss: 0.05535321265725294\n",
      "test loss: 0.053512913453588176\n",
      "STEP:  780\n",
      "loss: 0.0553530408415272\n",
      "test loss: 0.05351279365749756\n",
      "STEP:  781\n",
      "loss: 0.0553528693623297\n",
      "test loss: 0.05351267415702455\n",
      "STEP:  782\n",
      "loss: 0.05535269821908687\n",
      "test loss: 0.05351255495114581\n",
      "STEP:  783\n",
      "loss: 0.055352527411230625\n",
      "test loss: 0.05351243603883891\n",
      "STEP:  784\n",
      "loss: 0.055352356938180934\n",
      "test loss: 0.0535123174190836\n",
      "STEP:  785\n",
      "loss: 0.05535218679935944\n",
      "test loss: 0.053512199090861176\n",
      "STEP:  786\n",
      "loss: 0.055352016994200004\n",
      "test loss: 0.05351208105315438\n",
      "STEP:  787\n",
      "loss: 0.05535184752212169\n",
      "test loss: 0.05351196330494825\n",
      "STEP:  788\n",
      "loss: 0.05535167838255564\n",
      "test loss: 0.05351184584522816\n",
      "STEP:  789\n",
      "loss: 0.05535150957492302\n",
      "test loss: 0.05351172867298343\n",
      "STEP:  790\n",
      "loss: 0.05535134109865151\n",
      "test loss: 0.053511611787201846\n",
      "STEP:  791\n",
      "loss: 0.055351172953166036\n",
      "test loss: 0.053511495186875695\n",
      "STEP:  792\n",
      "loss: 0.05535100513788666\n",
      "test loss: 0.05351137887099711\n",
      "STEP:  793\n",
      "loss: 0.055350837652244016\n",
      "test loss: 0.05351126283855986\n",
      "STEP:  794\n",
      "loss: 0.055350670495661725\n",
      "test loss: 0.053511147088560744\n",
      "STEP:  795\n",
      "loss: 0.05535050366756283\n",
      "test loss: 0.053511031619996006\n",
      "STEP:  796\n",
      "loss: 0.055350337167369756\n",
      "test loss: 0.05351091643186526\n",
      "STEP:  797\n",
      "loss: 0.0553501709945119\n",
      "test loss: 0.05351080152316811\n",
      "STEP:  798\n",
      "loss: 0.05535000514840969\n",
      "test loss: 0.05351068689290691\n",
      "STEP:  799\n",
      "loss: 0.05534983962849375\n",
      "test loss: 0.053510572540084525\n",
      "STEP:  800\n",
      "loss: 0.055349674434176606\n",
      "test loss: 0.05351045846370592\n",
      "STEP:  801\n",
      "loss: 0.05534950956489493\n",
      "test loss: 0.053510344662777404\n",
      "STEP:  802\n",
      "loss: 0.05534934502006348\n",
      "test loss: 0.0535102311363066\n",
      "STEP:  803\n",
      "loss: 0.05534918079911543\n",
      "test loss: 0.05351011788330268\n",
      "STEP:  804\n",
      "loss: 0.05534901690146437\n",
      "test loss: 0.053510004902776395\n",
      "STEP:  805\n",
      "loss: 0.0553488533265411\n",
      "test loss: 0.05350989219373981\n",
      "STEP:  806\n",
      "loss: 0.055348690073767075\n",
      "test loss: 0.05350977975520533\n",
      "STEP:  807\n",
      "loss: 0.055348527142569784\n",
      "test loss: 0.053509667586189386\n",
      "STEP:  808\n",
      "loss: 0.05534836453236436\n",
      "test loss: 0.05350955568570713\n",
      "STEP:  809\n",
      "loss: 0.05534820224258638\n",
      "test loss: 0.053509444052777494\n",
      "STEP:  810\n",
      "loss: 0.0553480402726513\n",
      "test loss: 0.053509332686417964\n",
      "STEP:  811\n",
      "loss: 0.05534787862198628\n",
      "test loss: 0.05350922158565037\n",
      "STEP:  812\n",
      "loss: 0.05534771729001174\n",
      "test loss: 0.0535091107494951\n",
      "STEP:  813\n",
      "loss: 0.05534755627615688\n",
      "test loss: 0.05350900017697644\n",
      "STEP:  814\n",
      "loss: 0.05534739557983859\n",
      "test loss: 0.05350888986711899\n",
      "STEP:  815\n",
      "loss: 0.05534723520048956\n",
      "test loss: 0.053508779818948256\n",
      "STEP:  816\n",
      "loss: 0.055347075137525216\n",
      "test loss: 0.05350867003149154\n",
      "STEP:  817\n",
      "loss: 0.05534691539037178\n",
      "test loss: 0.053508560503778176\n",
      "STEP:  818\n",
      "loss: 0.05534675595845765\n",
      "test loss: 0.053508451234837356\n",
      "STEP:  819\n",
      "loss: 0.05534659684120157\n",
      "test loss: 0.053508342223701016\n",
      "STEP:  820\n",
      "loss: 0.055346438038023325\n",
      "test loss: 0.05350823346940135\n",
      "STEP:  821\n",
      "loss: 0.055346279548357816\n",
      "test loss: 0.05350812497097288\n",
      "STEP:  822\n",
      "loss: 0.05534612137162014\n",
      "test loss: 0.0535080167274509\n",
      "STEP:  823\n",
      "loss: 0.055345963507238256\n",
      "test loss: 0.05350790873787195\n",
      "STEP:  824\n",
      "loss: 0.055345805954637645\n",
      "test loss: 0.05350780100127394\n",
      "STEP:  825\n",
      "loss: 0.05534564871323842\n",
      "test loss: 0.053507693516696486\n",
      "STEP:  826\n",
      "loss: 0.055345491782462154\n",
      "test loss: 0.05350758628318042\n",
      "STEP:  827\n",
      "loss: 0.055345335161737334\n",
      "test loss: 0.05350747929976733\n",
      "STEP:  828\n",
      "loss: 0.05534517885049106\n",
      "test loss: 0.05350737256550074\n",
      "STEP:  829\n",
      "loss: 0.055345022848145804\n",
      "test loss: 0.05350726607942579\n",
      "STEP:  830\n",
      "loss: 0.05534486715411909\n",
      "test loss: 0.053507159840587605\n",
      "STEP:  831\n",
      "loss: 0.05534471176784212\n",
      "test loss: 0.05350705384803368\n",
      "STEP:  832\n",
      "loss: 0.05534455668873841\n",
      "test loss: 0.05350694810081322\n",
      "STEP:  833\n",
      "loss: 0.05534440191623192\n",
      "test loss: 0.053506842597975585\n",
      "STEP:  834\n",
      "loss: 0.055344247449744303\n",
      "test loss: 0.05350673733857213\n",
      "STEP:  835\n",
      "loss: 0.0553440932887068\n",
      "test loss: 0.05350663232165562\n",
      "STEP:  836\n",
      "loss: 0.05534393943253666\n",
      "test loss: 0.05350652754627939\n",
      "STEP:  837\n",
      "loss: 0.055343785880664256\n",
      "test loss: 0.053506423011499\n",
      "STEP:  838\n",
      "loss: 0.055343632632514295\n",
      "test loss: 0.05350631871637086\n",
      "STEP:  839\n",
      "loss: 0.05534347968751462\n",
      "test loss: 0.053506214659952374\n",
      "STEP:  840\n",
      "loss: 0.055343327045079896\n",
      "test loss: 0.05350611084130345\n",
      "STEP:  841\n",
      "loss: 0.05534317470464444\n",
      "test loss: 0.053506007259483644\n",
      "STEP:  842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.05534302266563256\n",
      "test loss: 0.053505903913555294\n",
      "STEP:  843\n",
      "loss: 0.0553428709274649\n",
      "test loss: 0.05350580080258108\n",
      "STEP:  844\n",
      "loss: 0.05534271948957667\n",
      "test loss: 0.053505697925625484\n",
      "STEP:  845\n",
      "loss: 0.05534256835138441\n",
      "test loss: 0.053505595281753714\n",
      "STEP:  846\n",
      "loss: 0.05534241751232022\n",
      "test loss: 0.05350549287003368\n",
      "STEP:  847\n",
      "loss: 0.05534226697180526\n",
      "test loss: 0.05350539068953293\n",
      "STEP:  848\n",
      "loss: 0.0553421167292717\n",
      "test loss: 0.05350528873932137\n",
      "STEP:  849\n",
      "loss: 0.05534196678413773\n",
      "test loss: 0.05350518701846955\n",
      "STEP:  850\n",
      "loss: 0.05534181713584008\n",
      "test loss: 0.053505085526050294\n",
      "STEP:  851\n",
      "loss: 0.05534166778380189\n",
      "test loss: 0.05350498426113673\n",
      "STEP:  852\n",
      "loss: 0.05534151872744601\n",
      "test loss: 0.05350488322280353\n",
      "STEP:  853\n",
      "loss: 0.05534136996620478\n",
      "test loss: 0.05350478241012775\n",
      "STEP:  854\n",
      "loss: 0.055341221499499216\n",
      "test loss: 0.05350468182218607\n",
      "STEP:  855\n",
      "loss: 0.055341073326766306\n",
      "test loss: 0.0535045814580581\n",
      "STEP:  856\n",
      "loss: 0.05534092544742591\n",
      "test loss: 0.05350448131682357\n",
      "STEP:  857\n",
      "loss: 0.055340777860910975\n",
      "test loss: 0.05350438139756399\n",
      "STEP:  858\n",
      "loss: 0.05534063056664102\n",
      "test loss: 0.05350428169936272\n",
      "STEP:  859\n",
      "loss: 0.055340483564055146\n",
      "test loss: 0.05350418222130325\n",
      "STEP:  860\n",
      "loss: 0.05534033685257864\n",
      "test loss: 0.05350408296247133\n",
      "STEP:  861\n",
      "loss: 0.05534019043163634\n",
      "test loss: 0.05350398392195492\n",
      "STEP:  862\n",
      "loss: 0.05534004430065873\n",
      "test loss: 0.053503885098840624\n",
      "STEP:  863\n",
      "loss: 0.05533989845907523\n",
      "test loss: 0.053503786492218944\n",
      "STEP:  864\n",
      "loss: 0.05533975290632027\n",
      "test loss: 0.053503688101180884\n",
      "STEP:  865\n",
      "loss: 0.05533960764181558\n",
      "test loss: 0.05350358992481866\n",
      "STEP:  866\n",
      "loss: 0.05533946266499572\n",
      "test loss: 0.0535034919622258\n",
      "STEP:  867\n",
      "loss: 0.05533931797528723\n",
      "test loss: 0.05350339421249725\n",
      "STEP:  868\n",
      "loss: 0.05533917357212482\n",
      "test loss: 0.0535032966747301\n",
      "STEP:  869\n",
      "loss: 0.05533902945493697\n",
      "test loss: 0.053503199348021\n",
      "STEP:  870\n",
      "loss: 0.05533888562315353\n",
      "test loss: 0.053503102231469646\n",
      "STEP:  871\n",
      "loss: 0.055338742076206315\n",
      "test loss: 0.053503005324176824\n",
      "STEP:  872\n",
      "loss: 0.055338598813528\n",
      "test loss: 0.05350290862524377\n",
      "STEP:  873\n",
      "loss: 0.05533845583454831\n",
      "test loss: 0.053502812133774294\n",
      "STEP:  874\n",
      "loss: 0.05533831313869745\n",
      "test loss: 0.05350271584887321\n",
      "STEP:  875\n",
      "loss: 0.05533817072541146\n",
      "test loss: 0.05350261976964585\n",
      "STEP:  876\n",
      "loss: 0.05533802859411937\n",
      "test loss: 0.05350252389520045\n",
      "STEP:  877\n",
      "loss: 0.05533788674425474\n",
      "test loss: 0.0535024282246452\n",
      "STEP:  878\n",
      "loss: 0.05533774517524982\n",
      "test loss: 0.05350233275709035\n",
      "STEP:  879\n",
      "loss: 0.055337603886543615\n",
      "test loss: 0.05350223749164829\n",
      "STEP:  880\n",
      "loss: 0.05533746287756426\n",
      "test loss: 0.05350214242743116\n",
      "STEP:  881\n",
      "loss: 0.05533732214774525\n",
      "test loss: 0.053502047563553694\n",
      "STEP:  882\n",
      "loss: 0.05533718169652036\n",
      "test loss: 0.05350195289913198\n",
      "STEP:  883\n",
      "loss: 0.055337041523327156\n",
      "test loss: 0.05350185843328287\n",
      "STEP:  884\n",
      "loss: 0.05533690162759784\n",
      "test loss: 0.05350176416512576\n",
      "STEP:  885\n",
      "loss: 0.055336762008769885\n",
      "test loss: 0.053501670093779576\n",
      "STEP:  886\n",
      "loss: 0.05533662266627591\n",
      "test loss: 0.05350157621836717\n",
      "STEP:  887\n",
      "loss: 0.05533648359955067\n",
      "test loss: 0.05350148253801121\n",
      "STEP:  888\n",
      "loss: 0.05533634480803368\n",
      "test loss: 0.05350138905183569\n",
      "STEP:  889\n",
      "loss: 0.055336206291158624\n",
      "test loss: 0.05350129575896692\n",
      "STEP:  890\n",
      "loss: 0.05533606804836514\n",
      "test loss: 0.05350120265853194\n",
      "STEP:  891\n",
      "loss: 0.055335930079085105\n",
      "test loss: 0.0535011097496597\n",
      "STEP:  892\n",
      "loss: 0.05533579238276169\n",
      "test loss: 0.05350101703148\n",
      "STEP:  893\n",
      "loss: 0.05533565495883089\n",
      "test loss: 0.053500924503125186\n",
      "STEP:  894\n",
      "loss: 0.05533551780672672\n",
      "test loss: 0.053500832163727766\n",
      "STEP:  895\n",
      "loss: 0.05533538092589289\n",
      "test loss: 0.053500740012422866\n",
      "STEP:  896\n",
      "loss: 0.05533524431576428\n",
      "test loss: 0.053500648048346235\n",
      "STEP:  897\n",
      "loss: 0.055335107975778015\n",
      "test loss: 0.053500556270635574\n",
      "STEP:  898\n",
      "loss: 0.05533497190537899\n",
      "test loss: 0.05350046467843007\n",
      "STEP:  899\n",
      "loss: 0.0553348361040051\n",
      "test loss: 0.05350037327086982\n",
      "STEP:  900\n",
      "loss: 0.055334700571095116\n",
      "test loss: 0.05350028204709736\n",
      "STEP:  901\n",
      "loss: 0.055334565306092175\n",
      "test loss: 0.05350019100625548\n",
      "STEP:  902\n",
      "loss: 0.05533443030843639\n",
      "test loss: 0.05350010014749\n",
      "STEP:  903\n",
      "loss: 0.05533429557756695\n",
      "test loss: 0.05350000946994674\n",
      "STEP:  904\n",
      "loss: 0.055334161112926904\n",
      "test loss: 0.053499918972773924\n",
      "STEP:  905\n",
      "loss: 0.05533402691396059\n",
      "test loss: 0.05349982865512101\n",
      "STEP:  906\n",
      "loss: 0.055333892980105014\n",
      "test loss: 0.05349973851613921\n",
      "STEP:  907\n",
      "loss: 0.05533375931080685\n",
      "test loss: 0.05349964855498091\n",
      "STEP:  908\n",
      "loss: 0.05533362590550922\n",
      "test loss: 0.05349955877080049\n",
      "STEP:  909\n",
      "loss: 0.05533349276365363\n",
      "test loss: 0.05349946916275309\n",
      "STEP:  910\n",
      "loss: 0.05533335988468883\n",
      "test loss: 0.0534993797299956\n",
      "STEP:  911\n",
      "loss: 0.05533322726805528\n",
      "test loss: 0.05349929047168721\n",
      "STEP:  912\n",
      "loss: 0.055333094913200084\n",
      "test loss: 0.053499201386987945\n",
      "STEP:  913\n",
      "loss: 0.05533296281956245\n",
      "test loss: 0.05349911247505941\n",
      "STEP:  914\n",
      "loss: 0.0553328309865975\n",
      "test loss: 0.053499023735064856\n",
      "STEP:  915\n",
      "loss: 0.0553326994137465\n",
      "test loss: 0.05349893516616941\n",
      "STEP:  916\n",
      "loss: 0.055332568100456794\n",
      "test loss: 0.053498846767539264\n",
      "STEP:  917\n",
      "loss: 0.055332437046173355\n",
      "test loss: 0.053498758538341844\n",
      "STEP:  918\n",
      "loss: 0.05533230625034715\n",
      "test loss: 0.05349867047774752\n",
      "STEP:  919\n",
      "loss: 0.05533217571242101\n",
      "test loss: 0.05349858258492667\n",
      "STEP:  920\n",
      "loss: 0.05533204543185487\n",
      "test loss: 0.053498494859052725\n",
      "STEP:  921\n",
      "loss: 0.05533191540808357\n",
      "test loss: 0.05349840729929856\n",
      "STEP:  922\n",
      "loss: 0.055331785640563555\n",
      "test loss: 0.053498319904840864\n",
      "STEP:  923\n",
      "loss: 0.055331656128744015\n",
      "test loss: 0.053498232674857295\n",
      "STEP:  924\n",
      "loss: 0.05533152687206953\n",
      "test loss: 0.05349814560852605\n",
      "STEP:  925\n",
      "loss: 0.05533139786999967\n",
      "test loss: 0.05349805870502808\n",
      "STEP:  926\n",
      "loss: 0.055331269121983086\n",
      "test loss: 0.0534979719635455\n",
      "STEP:  927\n",
      "loss: 0.05533114062746763\n",
      "test loss: 0.05349788538326197\n",
      "STEP:  928\n",
      "loss: 0.05533101238591018\n",
      "test loss: 0.05349779896336317\n",
      "STEP:  929\n",
      "loss: 0.05533088439675678\n",
      "test loss: 0.0534977127030351\n",
      "STEP:  930\n",
      "loss: 0.05533075665946612\n",
      "test loss: 0.05349762660146757\n",
      "STEP:  931\n",
      "loss: 0.0553306291734914\n",
      "test loss: 0.05349754065785013\n",
      "STEP:  932\n",
      "loss: 0.055330501938281464\n",
      "test loss: 0.053497454871374146\n",
      "STEP:  933\n",
      "loss: 0.05533037495330009\n",
      "test loss: 0.05349736924123409\n",
      "STEP:  934\n",
      "loss: 0.05533024821799199\n",
      "test loss: 0.05349728376662438\n",
      "STEP:  935\n",
      "loss: 0.055330121731820274\n",
      "test loss: 0.05349719844674108\n",
      "STEP:  936\n",
      "loss: 0.055329995494236735\n",
      "test loss: 0.05349711328078348\n",
      "STEP:  937\n",
      "loss: 0.055329869504697426\n",
      "test loss: 0.05349702826795156\n",
      "STEP:  938\n",
      "loss: 0.05532974376266271\n",
      "test loss: 0.053496943407446276\n",
      "STEP:  939\n",
      "loss: 0.055329618267587366\n",
      "test loss: 0.05349685869847131\n",
      "STEP:  940\n",
      "loss: 0.055329493018932877\n",
      "test loss: 0.053496774140231344\n",
      "STEP:  941\n",
      "loss: 0.055329368016154276\n",
      "test loss: 0.053496689731933326\n",
      "STEP:  942\n",
      "loss: 0.0553292432587105\n",
      "test loss: 0.05349660547278462\n",
      "STEP:  943\n",
      "loss: 0.05532911874606563\n",
      "test loss: 0.05349652136199575\n",
      "STEP:  944\n",
      "loss: 0.05532899447767499\n",
      "test loss: 0.053496437398778005\n",
      "STEP:  945\n",
      "loss: 0.055328870453000056\n",
      "test loss: 0.0534963535823447\n",
      "STEP:  946\n",
      "loss: 0.055328746671505176\n",
      "test loss: 0.05349626991191066\n",
      "STEP:  947\n",
      "loss: 0.05532862313264801\n",
      "test loss: 0.05349618638669247\n",
      "STEP:  948\n",
      "loss: 0.05532849983589002\n",
      "test loss: 0.05349610300590818\n",
      "STEP:  949\n",
      "loss: 0.05532837678069955\n",
      "test loss: 0.053496019768778276\n",
      "STEP:  950\n",
      "loss: 0.055328253966533854\n",
      "test loss: 0.05349593667452381\n",
      "STEP:  951\n",
      "loss: 0.055328131392863335\n",
      "test loss: 0.053495853722368064\n",
      "STEP:  952\n",
      "loss: 0.055328009059146574\n",
      "test loss: 0.053495770911535796\n",
      "STEP:  953\n",
      "loss: 0.055327886964857395\n",
      "test loss: 0.0534956882412545\n",
      "STEP:  954\n",
      "loss: 0.055327765109447515\n",
      "test loss: 0.0534956057107522\n",
      "STEP:  955\n",
      "loss: 0.05532764349239346\n",
      "test loss: 0.053495523319258304\n",
      "STEP:  956\n",
      "loss: 0.05532752211315791\n",
      "test loss: 0.053495441066005595\n",
      "STEP:  957\n",
      "loss: 0.055327400971207974\n",
      "test loss: 0.053495358950227036\n",
      "STEP:  958\n",
      "loss: 0.05532728006601492\n",
      "test loss: 0.05349527697115754\n",
      "STEP:  959\n",
      "loss: 0.05532715939704465\n",
      "test loss: 0.05349519512803495\n",
      "STEP:  960\n",
      "loss: 0.05532703896376534\n",
      "test loss: 0.05349511342009716\n",
      "STEP:  961\n",
      "loss: 0.055326918765646944\n",
      "test loss: 0.05349503184658473\n",
      "STEP:  962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.05532679880216305\n",
      "test loss: 0.05349495040673959\n",
      "STEP:  963\n",
      "loss: 0.05532667907278063\n",
      "test loss: 0.05349486909980609\n",
      "STEP:  964\n",
      "loss: 0.05532655957697542\n",
      "test loss: 0.05349478792502908\n",
      "STEP:  965\n",
      "loss: 0.05532644031420843\n",
      "test loss: 0.05349470688165661\n",
      "STEP:  966\n",
      "loss: 0.055326321283963635\n",
      "test loss: 0.053494625968937594\n",
      "STEP:  967\n",
      "loss: 0.055326202485710164\n",
      "test loss: 0.053494545186122344\n",
      "STEP:  968\n",
      "loss: 0.05532608391892379\n",
      "test loss: 0.05349446453246326\n",
      "STEP:  969\n",
      "loss: 0.055325965583069986\n",
      "test loss: 0.05349438400721549\n",
      "STEP:  970\n",
      "loss: 0.055325847477636206\n",
      "test loss: 0.05349430360963413\n",
      "STEP:  971\n",
      "loss: 0.05532572960209257\n",
      "test loss: 0.053494223338977596\n",
      "STEP:  972\n",
      "loss: 0.05532561195590975\n",
      "test loss: 0.05349414319450453\n",
      "STEP:  973\n",
      "loss: 0.05532549453857712\n",
      "test loss: 0.0534940631754778\n",
      "STEP:  974\n",
      "loss: 0.05532537734955698\n",
      "test loss: 0.05349398328115881\n",
      "STEP:  975\n",
      "loss: 0.05532526038833973\n",
      "test loss: 0.05349390351081336\n",
      "STEP:  976\n",
      "loss: 0.05532514365439778\n",
      "test loss: 0.05349382386370781\n",
      "STEP:  977\n",
      "loss: 0.05532502714721229\n",
      "test loss: 0.05349374433911004\n",
      "STEP:  978\n",
      "loss: 0.05532491086626279\n",
      "test loss: 0.05349366493629113\n",
      "STEP:  979\n",
      "loss: 0.055324794811024536\n",
      "test loss: 0.05349358565452204\n",
      "STEP:  980\n",
      "loss: 0.05532467898098861\n",
      "test loss: 0.05349350649307717\n",
      "STEP:  981\n",
      "loss: 0.05532456337563023\n",
      "test loss: 0.05349342745123149\n",
      "STEP:  982\n",
      "loss: 0.055324447994430065\n",
      "test loss: 0.05349334852826259\n",
      "STEP:  983\n",
      "loss: 0.05532433283688152\n",
      "test loss: 0.0534932697234488\n",
      "STEP:  984\n",
      "loss: 0.05532421790245622\n",
      "test loss: 0.053493191036071856\n",
      "STEP:  985\n",
      "loss: 0.0553241031906419\n",
      "test loss: 0.05349311246541441\n",
      "STEP:  986\n",
      "loss: 0.05532398870092653\n",
      "test loss: 0.053493034010759784\n",
      "STEP:  987\n",
      "loss: 0.05532387443279402\n",
      "test loss: 0.05349295567139495\n",
      "STEP:  988\n",
      "loss: 0.05532376038572988\n",
      "test loss: 0.053492877446607814\n",
      "STEP:  989\n",
      "loss: 0.05532364655922228\n",
      "test loss: 0.05349279933568826\n",
      "STEP:  990\n",
      "loss: 0.055323532952758096\n",
      "test loss: 0.0534927213379274\n",
      "STEP:  991\n",
      "loss: 0.055323419565825926\n",
      "test loss: 0.053492643452619316\n",
      "STEP:  992\n",
      "loss: 0.05532330639791669\n",
      "test loss: 0.053492565679058475\n",
      "STEP:  993\n",
      "loss: 0.05532319344851489\n",
      "test loss: 0.05349248801654283\n",
      "STEP:  994\n",
      "loss: 0.05532308071711411\n",
      "test loss: 0.053492410464370366\n",
      "STEP:  995\n",
      "loss: 0.05532296820320269\n",
      "test loss: 0.053492333021841955\n",
      "STEP:  996\n",
      "loss: 0.05532285590627729\n",
      "test loss: 0.05349225568826016\n",
      "STEP:  997\n",
      "loss: 0.05532274382582772\n",
      "test loss: 0.05349217846292928\n",
      "STEP:  998\n",
      "loss: 0.05532263196134477\n",
      "test loss: 0.05349210134515527\n",
      "STEP:  999\n",
      "loss: 0.055322520312326894\n",
      "test loss: 0.05349202433424621\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "nesterov = True\n",
    "\n",
    "# set random seed to 0\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# load data and make training set\n",
    "input = torch.from_numpy(xTr[:1000]).double()\n",
    "target = torch.from_numpy(yTr[:1000]).double()\n",
    "test_input = input[:100]\n",
    "test_target = target[:100]\n",
    "\n",
    "# build the model\n",
    "seq = Sequence()\n",
    "seq.double()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.SGD(seq.parameters(), lr=lr, momentum=momentum, nesterov=nesterov, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "\n",
    "#begin to train\n",
    "for i in range(n_epochs):\n",
    "    print('STEP: ', i)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        out = seq(input)\n",
    "        loss = criterion(out, target)\n",
    "        print('loss:', loss.item())\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    # begin to predict, no need to track gradient here\n",
    "    with torch.no_grad():\n",
    "        future = 32\n",
    "        pred = seq(test_input, future=future)\n",
    "        loss = criterion(pred[:, :-future], test_target)\n",
    "        print('test loss:', loss.item())\n",
    "        y = pred.detach().numpy()\n",
    "        losses.append(loss)\n",
    "        \n",
    "    # draw the result\n",
    "    if i % 10 == 0:\n",
    "        plt.figure(figsize=(30,10))\n",
    "        plt.title('Predict future values for time sequences\\n(Dashlines are predicted values)', fontsize=30)\n",
    "        plt.xlabel('x', fontsize=20)\n",
    "        plt.ylabel('y', fontsize=20)\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "        def draw(yi, color):\n",
    "            plt.plot(np.arange(input.size(1)), yi[:input.size(1)], color, linewidth = 2.0)\n",
    "            plt.plot(np.arange(input.size(1), input.size(1) + future), yi[input.size(1):], color + ':', linewidth = 2.0)\n",
    "        draw(y[0], 'r')\n",
    "        draw(y[1], 'g')\n",
    "        draw(y[2], 'b')\n",
    "        plt.savefig('predict%d.png'%i, bbox_inches='tight')\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses, 'r')\n",
    "plt.title('Testing loss throughout training')\n",
    "plt.ylabel('Cross-entropy loss', fontsize=20)\n",
    "plt.xlabel('Epoch', fontsize=20)\n",
    "plt.savefig('loss_1000epochs.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gpytorch]",
   "language": "python",
   "name": "conda-env-gpytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
